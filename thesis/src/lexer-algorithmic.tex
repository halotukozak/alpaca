\chapter{Algorytmy analizy leksykalnej}
\label{ch:lexer-algorithmic}


\section{Teoretyczne podstawy}
\label{sec:lexer-theory}

Analizator leksykalny (lekser) stanowi fundamentalną fazę przetwarzania tekstu źródłowego, przekształcając sekwencję znaków w~ciąg jednostek leksykalnych (tokenów), które reprezentują niepodzielne elementy składniowe dla fazy analizy składniowej~\cite{cornell-lexing}.
Klasyczna konstrukcja analizatora opiera się na~połączeniu teorii formalnych języków oraz teorii automatów skończonych~\cite{lexical-analysis-wiki}.

\subsection{Opis języka tokenów}
\label{subsec:lexer-theory-lang}
Każda klasa tokenów jest definiowana poprzez język regularny: zbiór słów akceptowanych przez wyrażenie regularne.
Zbiór reguł tokenów stanowi sumę języków regularnych; ich unia~jest również językiem regularnym~\cite{jezyki-formalne-automaty}, co~umożliwia kompilację ich do~jednolitego automatu deterministycznego.

\subsection{Automaty skończone}
\label{subsec:lexer-theory-dfa}
Wyrażenia regularne są transformowane do~postaci niedeterministycznej (NFA) poprzez konstrukcję Thompsona~\cite{arces2018thompson}.
Następnie deterministyczna postać automatu (DFA) konstruowana jest poprzez algorytm usuwania niedeterminizmu (ang.~\emph{powerset construction}), polegający na~iteracyjnym łączeniu zbiorów stanów NFA\@.
Opcjonalnie przeprowadza się minimalizację automatu poprzez usuwanie stanów równoważnych~\cite{jezyki-formalne-automaty}.

DFA przetwarza wejście znak po znaku, zachowując jednoznaczny stan aktywny oraz informując, czy aktualny prefiks odpowiada jednemu z~zdefiniowanych tokenów.

\subsection{Strategia wyboru dopasowania}
\label{subsec:lexer-theory-matching}
Lekser stosuje dwie komplementarne zasady determinujące zachowanie dla wieloznacznych sytuacji:

\begin{itemize}
    \item Dopóki DFA ma ścieżkę przejść, znak jest konsumowany; token jest emitowany dopiero po~ostatnim stanie akceptującym widzianym na tej ścieżce (najdłuższe dopasowanie, ang. \textit{maximal munch}).
    \item Gdy kilka reguł akceptuje prefiks o~tej samej długości, wybierana jest reguła o najwyższym priorytecie (często określanym kolejnością definicji).
\end{itemize}

Kombinacja tych zasad gwarantuje deterministyczną oraz reproducywalną sekwencję tokenów bez~konieczności specjalnych mechanizmów rozstrzygania konfliktów.

\subsection{Błędy leksykalne}
\label{subsec:lexer-theory-errors}
W~sytuacji, gdy automat nie posiada przejścia dla bieżącego znaku, ogłaszany jest błąd leksykalny w~bieżącej pozycji wejścia.
Mechanizm diagnostyczny przywołuje informacje o~pozycji znaku, co~znacząco ułatwia śledzenie źródła problemu w~tekście źródłowym.


\section[Automaty DFA a wyrażenia regularne w systemie ALPACA]{Automaty DFA a wyrażenia regularne w~systemie ALPACA}
\label{sec:dfa-vs-regex}

\subsection{Tło: Tradycyjne podejście}
\label{subsec:traditional-dfa-approach}

Narzędzia klasyczne do~budowy leksera (takie jak~Lex~\cite{lesk1975lex}) generują jawny, deterministyczny automat skończony (DFA) z~zestawu wyrażeń regularnych.
Podejście to wymaga implementacji pełnego zestawu algorytmów: transformacja NFA$\to$DFA, minimalizacja, optymalizacja — każdy etap jest czasochłonny dla twórcy narzędzia.

\subsection{Alternatywa: Wyrażenia regularne biblioteczne}
\label{subsec:regex-alternative}

W~systemie ALPACA podejście tradycyjne zostało zastąpione mechanizmem wykorzystującym natywny silnik wyrażeń regularnych biblioteki standardowej Scali~\cite{scala-regex-api}.
Zamiast ręcznie kodować DFA, makro kompilacyjne łączy wszystkie wzorce operatorem alternatywy (\texttt{|}) w~jedno wyrażenie regularne o~nazwanych grupach, umożliwiając rozróżnienie, która reguła dopasowała się podczas każdego przebiegu.

\subsection{Zalety podejścia opartego na wyrażeniach regularnych}
\label{subsec:pattern-matching-pros}
\begin{itemize}
    \item Poprzez wykorzystanie powszechnie znanego mechanizmu wyrażeń regularnych, użytkownicy języka specjalistycznego (DSL) mogą definiować reguły leksykalne bez~konieczności zrozumienia złożoności konstrukcji automatu i~algorytmów optymalizacji.
    \item Użytkownicy mogą zastosować rozszerzenia wyrażeń regularnych (takie jak~\emph{backreference}, \emph{negative lookahead}, czy~warunkowość), których implementacja byłaby niemożliwa w~jawnym DFA\@.
    \item Własna implementacja DFA wymaga pokrycia pełnego spektrum funkcjonalności wyrażeń regularnych, ciągłego utrzymania w~synchronizacji z~ewolucją języka hosta, oraz inwestycji w~optymalizację.
    ALPACA deleguje ten wysiłek do~zoptymalizowanego i~wielokrotnie przetestowanego silnika bibliotecznego.
    \item Definicje reguł leksykalnych pozostają kompaktowe i~czytelne, co~ułatwia przegląd, weryfikację i~modyfikację.
\end{itemize}

\subsection{Wady i ograniczenia}
\label{subsec:pattern-matching-cons}

\begin{itemize}
    \item Silniki wyrażeń regularnych mogą wykazywać wyższą złożoność obliczeniową niż ręcznie zoptymalizowane DFA, zwłaszcza w~przypadku dużych zbiorów reguł lub~złożonych wzorców.
    \item Zachowanie silnika wyrażeń regularnych dla dwuznacznych sytuacji (na~przykład gdy dwa wzorce różnej długości akceptują identyczną sekwencję) zależy od~implementacji silnika.
    W~niekorzystnych przypadkach może prowadzić do~nieoczekiwanych wyborów.
    Rozwiązaniem jest jawna deklaracja priorytetu poprzez kolejność definiowania reguł.
\end{itemize}

\subsection{Syntetyzacja: Decyzja projektu ALPACA}
\label{subsec:alpaca-design-choice}

Powyższe rozważania doprowadziły do~wyboru wyrażeń regularnych nad jawną konstrukcją DFA\@.
Wymiana wydajności (potencjalnie) za~uproszczenie interfejsu jest~akceptowalna w~kontekście systemu ALPACA\@.


\section[Praktyczna implementacja leksera w systemie ALPACA]{Praktyczna implementacja leksera w~systemie ALPACA}
\label{sec:lexer-impl}

Implementacja modułu \texttt{alpaca.lexer} łączy ergonomię języka specjalistycznego (DSL) z~kodem wykonywany w~czasie kompilacji, eliminując narzut parsowania wyrażeń regularnych w~czasie działania aplikacji.

\subsection[Przebieg tokenizacji w ALPACA]{Przebieg tokenizacji w~ALPACA}
\label{subsec:lexer-impl-flow}

Podczas kompilacji projektu wszystkie wzorce są łączone operatorem alternatywy (\texttt{|}) w~jedno wyrażenie regularne z~nazwanymi grupami.
Mechanizm ten umożliwia rozróżnienie, która z~reguł dopasowała się podczas każdego przebiegu wyszukiwania.
Następnie pętla skanująca—podczas wykonania aplikacji iteracyjnie wywołuje to~wyrażenie na~kolejnych fragmentach wejścia, identyfikuje dopasowany token, akumuluje leksemy oraz przesuwa wskaźnik wejścia aż do~wyczerpania danych.

\subsection{Obsługa reguł ignorowanych}
\label{subsec:lexer-impl-ignored}

System ALPACA umożliwia oznaczenie reguł jako ,,ignorowanych''.
Takie reguły są wbudowywane we wspólny wzorzec, jednak ich dopasowania nie~generują tokenów wyjściowych.
Ujednolicony przebieg pętli skanującej upraszcza kod: ignorowane tokeny różnią się od~normalnych wyłącznie akcją podejmowaną po~dopasowaniu (brak emisji leksemu, zamiast tego aktualizacja stanu wewnętrznego).

\subsection[Stanowa analiza leksykalna i rozszerzenia kontekstu]{Stanowa analiza leksykalna i~rozszerzenia kontekstu}
\label{subsec:lexer-impl-context}
Możliwa jest stanowa analiza leksykalna poprzez utrzymywanie stanu maszyny stanów w~obiekcie kontekstu.
System ALPACA wewnętrznie definiuje mechanizm \texttt{BetweenStages}, który jest wywoływany po~każdym rozpoznaniu leksemu i~umożliwia modyfikację stanu kontekstu.
Domyślna implementacja rejestruje ostatni leksem oraz śledzi numer linii i~kolumny; użytkownik może jednak rozszerzyć tę~logikę o~własne zachowania, na~przykład weryfikację poprawności zagnieżdżenia nawiasów.

\subsection{Diagnostyka błędów leksykalnych}
\label{subsec:lexer-impl-errors}
Gdy algorytm skanowania nie~odnajduje prefiksu (brak przejścia w~automacie), lekser zgłasza błąd leksykalny.
Mechanizm \texttt{BetweenStages} umożliwia zbieranie danych kontekstowych (numer linii, kolumny, ostatni prawidłowy leksem), które następnie wzbogacają raport diagnostyczny.
To~podejście znacząco poprawia doświadczenie użytkownika podczas debugowania błędów składniowych.

\subsection{Strumieniowe przetwarzanie wejścia}
\label{subsec:lexer-impl-lazy-reader}
W~celu unikania nadmiernego zużycia pamięci, lekser analizuje wejście w~sposób strumieniowy poprzez implementację interfejsu \texttt{CharSequence}.
Klasa \texttt{LazyReader} realizuje tę~funkcjonalność: pobiera dane ze~źródła w~blokach o~rozmiarze 16~KB (ang.~\emph{chunks}) i~buforuje je lokalnie.
Metoda \texttt{ensure} zapewnia, że~żądana pozycja jest~dostępna w~buforze, czytając kolejne porcje danych w~razie potrzeby.

\lstinputlisting[language=scala,caption={Implementacja metody \texttt{ensure} w~klasie \texttt{LazyReader}},label={lst:lazy-reader-ensure-impl}]{listings/chapter4/lazy-reader-ensure-impl.scala}

\subsection{Wczesna walidacja wzorców}
\label{subsec:lexer-impl-regexchecker}
Przed wygenerowaniem automatu skanującego, makro kompilacyjne uruchamia moduł \texttt{RegexChecker}, który analizuje wzorce pod~względem potencjalnych konfliktów.
Mechanizm ten wykrywa dwa klasy problemów:
\begin{itemize}
    \item Subsumpcję --- sytuację, w~której każde słowo akceptowane przez późniejszy wzorzec jest~również akceptowane przez wcześniejszy wzorzec, czyniąc~późniejszy \enquote{martwym kodem}.
    Przykład: jeśli definiuje się \texttt{ID~=~[a-z]+}, a~następnie \texttt{KEYWORD~=~if|then}, to~wszystkie słowa kluczowe będą dopasowane przez~\texttt{ID}, co~uniemożliwi rozpoznanie \texttt{KEYWORD}.
    \item Pokrycie prefiksów --- sytuację, w~której jeden wzorzec akceptuje prefiks słowa akceptowanego przez inny wzorzec.
    Przykład: jeśli zdefiniowano \texttt{LT~=~<} oraz \texttt{LE~=~<}\texttt{=}, lekser może dopasować \texttt{<}, a~następnie zgłosić błąd leksykalny dla~pozostałego znaku \texttt{=}, zamiast rozpoznać \texttt{<}\texttt{=}.
    Jest to konsekwencja użycia silnika wyrażeń regularnych, w którym zasada najdłuższego dopasowania (\textit{maximal munch}) nie ma zastosowania.
\end{itemize}

Wykrycie któregokolwiek z~powyższych problemów powoduje przerwanie kompilacji z~czytelnym i~działającym komunikatem diagnostycznym, dzięki~czemu konfiguracyjne błędy są~eliminowane przed~czasem wykonania programu.

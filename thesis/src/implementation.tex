\chapter{Implementacja}
\label{ch:implementacja}


\section{Praktyczna implementacja analizatora leksykalnego z wykorzystaniem makr w Scali 3}\label{sec:praktyczna-implementacja-analizatora-leksykalnego-z-wykorzystaniem-makr-w-scali-3}

\subsection{Wprowadzenie do studium przypadku}\label{subsec:wprowadzenie-do-studium-przypadku}

Niniejszy rozdział prezentuje praktyczną implementację systemu analizy leksykalnej (leksera) wykorzystującego zaawansowane mechanizmy metaprogramowania Scali 3.
Przedstawiony kod stanowi przykład zastosowania technik opisanych w poprzednim rozdziale do rozwiązania rzeczywistego problemu inżynierskiego: automatycznej generacji wydajnego analizatora leksykalnego z definicji wysokopoziomowej w formie języka dziedzinowego (DSL).

System \texttt{alpaca.lexer} implementuje transformację deklaratywnych reguł tokenizacji zapisanych jako funkcja częściowa (ang. \textit{partial function}) w kod procedualny wykonywany w czasie kompilacji.
Wykorzystuje przy tym pełne spektrum możliwości refleksji TASTy, włączając generację klas w czasie kompilacji, transformację drzew AST oraz wyspecjalizowane typy refinement.

\subsection{Architektura systemu leksera}\label{subsec:architektura-systemu-leksera}

\subsubsection{Interfejs użytkownika}\label{subsubsec:interfejs-uzytkownika}

System oferuje użytkownikowi przejrzysty interfejs DSL oparty na dopasowaniu wzorców:

\lstinputlisting[language=scala,caption={Definicja typu LexerDefinition},label={lst:lexer-01-lexerdefinition}, linerange={21}]{../../src/alpaca/lexer/Lexer.scala}

Definicja \texttt{LexerDefinition} reprezentuje reguły leksera jako funkcję częściową mapującą wzorce wyrażeń regularnych (jako ciągi znaków) na definicje tokenów.
Wykorzystanie funkcji częściowej pozwala na naturalne wyrażenie reguł leksykalnych w idiomatycznej składni Scali.

Główny punkt wejścia systemu stanowi metoda \texttt{lexer}:

\lstinputlisting[language=scala,caption={Punkt wejścia: transparent inline def lexer},label={lst:lexer-02-entrypoint}, linerange={44-52}]{../../src/alpaca/lexer/Lexer.scala}

Modyfikator \texttt{transparent inline} zapewnia, że zwracany typ będzie dokładnie odpowiadał wygenerowanej strukturze, włączając typy refinement dla poszczególnych tokenów.
Użycie parametrów kontekstowych (\texttt{using}) realizuje wzorzec dependency injection na poziomie systemu typów.

\subsubsection{Implementacja makra}\label{subsubsec:implementacja-makra}

Makro przyjmuje wyrażenie reprezentujące reguły leksera jako \texttt{Expr[Ctx ?=> LexerDefinition[Ctx]]} oraz instancje kontekstualnych klas pomocniczych.
Parametr \texttt{using Quotes} dostarcza dostępu do API refleksji TASTy.

\subsection{Analiza drzewa składni abstrakcyjnej}\label{subsec:analiza-drzewa-skladni-abstrakcyjnej}

\subsubsection{Dekonstrukcja funkcji częściowej}\label{subsubsec:dekonstrukcja-funkcji-czesciowej}

Kluczowym krokiem implementacji jest ekstrakcja reguł z definicji funkcji częściowej:

\lstinputlisting[language=scala,caption={Dekonstrukcja funkcji częściowej (dopasowanie AST do CaseDef)},label={lst:lexer-04-extract-cases}, linerange={72}]{../../src/alpaca/lexer/Lexer.scala}

Ten fragment kodu wykorzystuje dopasowanie wzorców w \textit{quotes} do dekonstrukcji typowanego AST funkcji częściowej.
Struktura \texttt{Lambda(\_, Match(\_, cases))} odpowiada wewnętrznej reprezentacji funkcji częściowej, gdzie \texttt{Match} zawiera listę przypadków \texttt{CaseDef}.

\subsection{Transformacja i adaptacja referencji}\label{subsec:transformacja-i-adaptacja-referencji}

\subsubsection{Klasa replacerefs}\label{subsubsec:klasa-replacerefs}

Kluczową techniką jest zastąpienie referencji do starego kontekstu nowymi referencjami:

\lstinputlisting[language=scala,caption={Zastąpienie referencji starego kontekstu nowymi (ReplaceRefs)},label={lst:lexer-06-replace-with-new-ctx}, linerange={75-78}]{../../src/alpaca/lexer/Lexer.scala}

Transformacja ta realizuje proces znany jako \("\)re-owning\("\) w terminologii kompilatorów — zmianę właściciela (owner) symboli w AST. Jest to konieczne, ponieważ kod oryginalnie odnoszący się do parametru makra musi zostać przepisany, aby odnosił się do parametru metody w wygenerowanej klasie.

Klasa \texttt{ReplaceRefs} udostępnia \texttt{TreeMap}, który podczas przejścia po AST podmienia referencje do wskazanych symboli na podane termy.
Umożliwia to tzw. re-owning — przeniesienie fragmentów kodu między różnymi właścicielami symboli bez ręcznego przepisywania drzew (por. \ref{subsubsec:core-replacerefs}).

\subsection{Ekstrakcja i kompilacja wzorców}\label{subsec:ekstrakcja-i-kompilacja-wzorcow}

\subsubsection{Funkcja extractSimple}\label{subsubsec:funkcja-extractsimple}

Funkcja \texttt{extractSimple} implementuje logikę dopasowania różnych typów definicji tokenów:

\lstinputlisting[language=scala,caption={Funkcja extractSimple: dopasowywanie definicji tokenów},label={lst:lexer-08-extract-simple}, linerange={80-85,99-108}]{../../src/alpaca/lexer/Lexer.scala}

Wykorzystuje ona dopasowanie wzorców w \textit{quotes} z ekstraktorem typów, umożliwiając rozróżnienie różnych wariantów definicji tokenów na poziomie typów.
Konstrukcja \texttt{type t <: ValidName} w wzorcu wiąże parametr typu do zmiennej wzorca \texttt{t}, umożliwiając jego późniejsze wykorzystanie.

\subsection{Analiza wzorców: klasa CompileNameAndPattern}
\label{subsec:compile-name-pattern}

Klasa \texttt{CompileNameAndPattern} stanowi kluczowy komponent systemu analizy leksykalnej, odpowiedzialny za ekstrakcję i walidację wzorców tokenów podczas ekspansji makra.
Jej głównym zadaniem jest transformacja różnorodnych form wzorców występujących w definicjach DSL na ujednolicone struktury \texttt{TokenInfo}, które następnie są wykorzystywane do generacji finalnego kodu leksera.

Implementacja wykorzystuje rekurencyjne przetwarzanie drzewa AST z zastosowaniem optymalizacji rekurencji ogonowej (\texttt{@tailrec}), co zapewnia efektywność działania nawet dla złożonych wzorców z wieloma alternatywami.

\subsection{Generacja klasy anonimowej}\label{subsec:generacja-klasy-anonimowej}

Anonimowa klasa implementująca \texttt{Tokenization[Ctx]} jest konstruowana programatycznie: (1) tworzymy symbol klasy przez \texttt{Symbol.newClass} wraz z listą deklaracji pól i typów; (2) budujemy ciało klasy (\texttt{ClassDef}) zawierające \texttt{ValDef} dla każdego zdefiniowanego tokena oraz pola \texttt{tokens} i \texttt{byName}; (3) określamy rodzica przez wywołanie konstruktora \texttt{Tokenization[Ctx]} z wymaganymi zależnościami; (4) instancjonujemy klasę i nadajemy jej typ zrafinowany przez kolejne \texttt{Refinement} odpowiadające polom-tokenom.

\lstinputlisting[language=scala,caption={Generacja i instancjowanie anonimowej klasy \texttt{Tokenization[Ctx]} z typem rafinowanym},label={lst:lexer-14-anon-class}, linerange={192-264}]{../../src/alpaca/lexer/Lexer.scala}

\subsubsection{Typy refinement}\label{subsubsec:typy-refinement}

Zwracany typ jest stopniowo rafinowany dla każdego tokena:

\lstinputlisting[language=scala,caption={Rafinowanie typu wynikowego o pola tokenów},label={lst:lexer-15-refinements}, linerange={253-258}]{../../src/alpaca/lexer/Lexer.scala}

\texttt{Refinement(tpe, name, memberType)} tworzy typ refinement dodający członka o podanej nazwie i typie do bazowego typu.
Pozwala to kompilatorowi śledzić, że zwrócony obiekt ma pola odpowiadające poszczególnym tokenom, umożliwiając dostęp do nich z pełnym wsparciem systemu typów.

\subsection{Walidacja i obsługa błędów}\label{subsec:walidacja-i-obsuga-bedow}

\subsubsection{Walidacja wzorców regularnych}\label{subsubsec:walidacja-wzorcow-regularnych}

System wykorzystuje pomocniczą klasę \texttt{RegexChecker} do walidacji wzorców:

Poniższy mechanizm sprawdza poprawność składni wyrażeń regularnych już w czasie kompilacji i raportuje błędy z dokładną lokalizacją wzorca.
Metoda \texttt{report.errorAndAbort} jest częścią API kompilatora do raportowania błędów w czasie kompilacji.
Przerwanie kompilacji w przypadku niepoprawnych wzorców zapewnia, że błędy konfiguracji są wykrywane możliwie wcześnie.

\subsubsection{Obsługa nieobsługiwanych konstrukcji}\label{subsubsec:obsuga-nieobsugiwanych-konstrukcji}

Kod jawnie sygnalizuje nieobsługiwane przypadki:
Obsługiwane są wyłącznie jasno zdefiniowane formy wzorców; w przypadku napotkania innej konstrukcji kompilacja jest przerywana z komunikatem zawierającym szczegóły AST, co upraszcza diagnostykę i utrzymuje zasadę fail-fast.
Ta strategia jest zgodna z zasadą fail-fast - lepiej jest wyraźnie odrzucić nieobsługiwane konstrukcje niż milcząco generować niepoprawny kod.


\section{Praktyczna implementacja generatora parserów z wykorzystaniem makr w Scali 3}\label{sec:praktyczna-implementacja-generatora-parserow-z-wykorzystaniem-makr-w-scali-3}

\subsection{Wprowadzenie do generatora parserów}\label{subsec:wprowadzenie-do-generatora-parserow}

System \texttt{alpaca.parser} stanowi zaawansowaną implementację generatora parserów LR(1), wykorzystującego metaprogramowanie Scali 3 do automatycznej konstrukcji tabel parsowania w czasie kompilacji.
W~przeciwieństwie do tradycyjnych generatorów parserów takich jak Yacc\cite{johnson1975yacc}, które generują kod w~osobnym kroku kompilacji, system ALPACA integruje generację parsera bezpośrednio w~procesie kompilacji Scali, oferując pełne wsparcie systemu typów oraz natychmiastową walidację gramatyk.

Implementacja parsera reprezentuje znacznie bardziej złożone wyzwanie inżynierskie niż lekser, ze względu na konieczność:
(1)~konstrukcji automatów LR(1) i tabel parsowania,
(2)~obsługi konfliktów shift-reduce i reduce-reduce,
(3)~generacji akcji semantycznych zachowujących bezpieczeństwo typów,
(4)~oraz radzenia sobie z ograniczeniami platformy JVM dotyczącymi rozmiaru metod.

\subsection{Interfejs API parsera}\label{subsec:interfejs-api-parsera}

\subsubsection{Definicja parsera}\label{subsubsec:definicja-parsera}

Użytkownik definiuje parser poprzez dziedziczenie po klasie bazowej \texttt{Parser[Ctx]}:

\lstinputlisting[language=scala,caption={Klasa bazowa Parser},label={lst:parser-01-base}, linerange={31-36}]{../../src/alpaca/parser/Parser.scala}

Typ parametryczny \texttt{Ctx} reprezentuje globalny kontekst parsera, umożliwiający przechowywanie stanu między akcjami semantycznymi (np.\ tablicę symboli).
Parametr kontekstualny \texttt{tables: Tables[Ctx]} jest automatycznie generowany przez makro i zawiera tabele parsowania oraz akcji semantycznych.

\subsubsection{Definicja reguł gramatycznych}\label{subsubsec:definicja-regul-gramatycznych}

Reguły gramatyczne definiowane są jako wartości typu \texttt{Rule[R]}, gdzie \texttt{R} określa typ wyniku redukcji:

\begin{lstlisting}[language=scala,caption={Przykład definicji reguł parsera},label={lst:parser-02-rules}]
object CalcParser extends Parser[EmptyGlobalCtx] {
  val Expr: Rule[Int] = rule(
    { case (Expr(a), Lexer.PLUS(_), Expr(b)) => a + b },
    { case Lexer.NUMBER(n) => n.value }
  )
  
  val root = rule { case Expr(result) => result }
}
\end{lstlisting}

Składnia wykorzystuje dopasowanie wzorców Scali do wyrażenia produkcji gramatycznych.
Każdy przypadek (\texttt{case}) reprezentuje pojedynczą produkcję, gdzie lewa strona wzorca odpowiada prawej stronie produkcji gramatycznej, a wyrażenie po strzałce~(\texttt{=>}) definiuje akcję semantyczną.
Na przykład wzorzec \texttt{\{ case (Expr(a), Lexer.PLUS(\_), Expr(b)) => a + b \}} odpowiada produkcji \texttt{Expr → Expr PLUS Expr} z~akcją sumującą wartości podwyrażeń.

\subsection{Generacja tabel parsowania w czasie kompilacji}\label{subsec:generacja-tabel-parsowania-w-czasie-kompilacji}

\subsubsection{Makro createTablesImpl}\label{subsubsec:makro-createtablesimpl}

Centralnym elementem systemu jest makro \texttt{createTablesImpl}, które analizuje definicję parsera i generuje tabele w czasie kompilacji:

\lstinputlisting[language=scala,caption={Sygnatura makra createTablesImpl},label={lst:parser-03-create-tables}, linerange={61-65}]{../../src/alpaca/parser/createTables.scala}

Makro wykonuje następujące kroki:
\begin{enumerate}
  \item Ekstrakcja wszystkich reguł gramatycznych z definicji parsera poprzez refleksję TASTy
  \item Transformacja wzorców dopasowania na produkcje gramatyczne
  \item Konstrukcja automatów LR(1) i tabel parsowania
  \item Generacja tabel akcji semantycznych
  \item Walidacja gramatyki i rozwiązywanie konfliktów
\end{enumerate}

\subsubsection{Ekstrakcja produkcji z wzorców}\label{subsubsec:ekstrakcja-produkcji-z-wzorcow}

Funkcja \texttt{extractEBNF} dokonuje transformacji wzorców dopasowania na produkcje gramatyczne:

\lstinputlisting[language=scala,caption={Ekstrakcja produkcji gramatycznych z wzorców},label={lst:parser-04-extract-ebnf}, linerange={80-95}]{../../src/alpaca/parser/createTables.scala}

Kluczowym wyzwaniem jest zachowanie poprawności referencji do symboli przy przenoszeniu kodu akcji semantycznej z kontekstu definicji reguły do wygenerowanej tabeli akcji.
Wymaga to zastosowania techniki \textit{re-owning} symboli, realizowanej przez klasę \texttt{ReplaceRefs}.

\subsection{Trudne problemy rozwiązane w implementacji}\label{subsec:trudne-problemy-rozwiazane-w-implementacji}

\subsubsection{Problem ograniczenia rozmiaru metod JVM}\label{subsubsec:problem-ograniczenia-rozmiaru-metod-jvm}

Jednym z najbardziej interesujących wyzwań technicznych napotkanych podczas implementacji było ograniczenie rozmiaru metod w maszynach wirtualnych JVM.
Zgodnie ze specyfikacją JVM\cite{lindholm2014java}, rozmiar kodu bajtowego pojedynczej metody nie może przekroczyć 65536 bajtów (64~KB).
Dla złożonych gramatyk z dużą liczbą stanów i produkcji, wygenerowane tabele parsowania mogą zawierać tysiące wpisów, co przy naiwnej implementacji prowadziło do przekroczenia tego limitu.

Problem manifestował się podczas próby wyrażenia tabeli parsowania jako literału mapowego w kodzie:

\begin{lstlisting}[language=scala,caption={Naiwna implementacja prowadząca do przekroczenia limitu},label={lst:parser-05-naive}]
// Ta implementacja generuje jeden duży literal mapy
'{ Map(
  (0, Terminal("PLUS")) -> Shift(1),
  (0, Terminal("NUMBER")) -> Shift(2),
  // ... tysiące kolejnych wpisów ...
  (999, Terminal("EOF")) -> Reduction(prod)
)}
\end{lstlisting}

Takie podejście generuje pojedynczą, dużą metodę zawierającą wszystkie wpisy tabeli, co dla gramatyk o~rozmiarze produkcyjnym skutkuje błędem kompilacji \texttt{java.lang.ClassFormatError: Code length too large}.

\textbf{Rozwiązanie:} Problem został rozwiązany poprzez zastosowanie techniki \textit{fragmentacji metod}.
Zamiast generować jeden duży literal mapy, każdy wpis tabeli jest dodawany w osobnej, małej metodzie pomocniczej:

\lstinputlisting[language=scala,caption={Rozwiązanie problemu rozmiaru metod przez fragmentację},label={lst:parser-06-fragment}, linerange={182-189}]{../../src/alpaca/parser/ParseTable.scala}

W tym podejściu:
\begin{itemize}
  \item Tworzymy builder mapy jako zmienną lokalną (\texttt{Map.newBuilder})
  \item Każde dodanie wpisu do buildera jest opakowane w osobną metodę \texttt{avoidTooLargerMethod()}
  \item Metody te są wywoływane sekwencyjnie jako lista wyrażeń w bloku
  \item Końcowy wynik jest uzyskiwany przez wywołanie \texttt{builder.result()}
\end{itemize}

Ta technika skutecznie obchodzi limit rozmiaru metody, ponieważ każda pomocnicza metoda zawiera tylko kilka instrukcji bajtowych, niezależnie od rozmiaru całej tabeli.
Dodatkowo, kompilator JIT może efektywnie zoptymalizować i zinlinować te małe metody w czasie wykonania, eliminując narzut wydajnościowy.

Rozwiązanie to ilustruje ważną lekcję w metaprogramowaniu: kod generowany przez makra musi respektować wszystkie ograniczenia platformy docelowej, które normalnie są niewidoczne dla programistów piszących kod ręcznie.

\subsubsection{Zachowanie bezpieczeństwa typów w akcjach semantycznych}\label{subsubsec:zachowanie-bezpieczenstwa-typow-w-akcjach-semantycznych}

Kolejnym istotnym wyzwaniem jest zapewnienie bezpieczeństwa typów w akcjach semantycznych podczas transformacji kodu z kontekstu makra do wygenerowanych tabel.
Akcje semantyczne definiowane przez użytkownika mogą odwoływać się do:
\begin{itemize}
  \item Kontekstu parsera (\texttt{ctx})
  \item Wartości z dopasowanych symboli gramatycznych
  \item Zewnętrznych funkcji i wartości
\end{itemize}

Problem polega na tym, że te referencje muszą zostać przepisane podczas przenoszenia kodu akcji z miejsca definicji do tabeli akcji.
Funkcja \texttt{createAction} realizuje tę transformację:

\lstinputlisting[language=scala,caption={Tworzenie akcji semantycznej z zachowaniem referencji},label={lst:parser-07-create-action}, linerange={82-95}]{../../src/alpaca/parser/createTables.scala}

Kluczowe aspekty implementacji:
\begin{enumerate}
  \item \textbf{Parametryzacja akcji:} Akcja jest transformowana w funkcję przyjmującą kontekst (\texttt{ctx}) oraz listę dzieci w drzewie parsowania (\texttt{param})
  \item \textbf{Re-owning symboli:} Referencje do kontekstu parsera są zastępowane parametrem funkcji
  \item \textbf{Ekstrakcja wartości:} Wartości z dopasowanych symboli są ekstrahowane z listy dzieci poprzez indeksowanie
  \item \textbf{Rzutowanie typów:} System typów zapewnia, że ekstrakcje są bezpieczne względem typów dzięki informacji z wzorca dopasowania
\end{enumerate}

\subsubsection{Rozwiązywanie konfliktów gramatycznych}\label{subsubsec:rozwiazywanie-konfliktow-gramatycznych}

Parser LR może napotkać konflikty typu shift-reduce lub reduce-reduce podczas konstrukcji tabel parsowania.
System ALPACA oferuje deklaratywny mechanizm rozwiązywania takich konfliktów poprzez relacje precedencji:

\begin{lstlisting}[language=scala,caption={Deklaracja rozwiązań konfliktów},label={lst:parser-08-conflicts}]
override val resolutions = Set(
  P.ofName("times").before(Lexer.PLUS),
  P.ofName("plus").after(Lexer.TIMES)
)
\end{lstlisting}

Implementacja wykorzystuje klasę \texttt{ConflictResolutionTable}, która podczas konstrukcji tabeli parsowania:
\begin{enumerate}
  \item Wykrywa konflikty między akcjami dla danego stanu i symbolu
  \item Konsultuje zdefiniowane przez użytkownika relacje precedencji
  \item Wybiera odpowiednią akcję zgodnie z deklaracją
  \item Zgłasza błąd kompilacji dla nierozwiązanych konfliktów
\end{enumerate}

To podejście umożliwia wyrażenie precedencji i łączności operatorów w sposób bardziej naturalny niż tradycyjne deklaracje \texttt{\%left}, \texttt{\%right} i~\texttt{\%nonassoc} w Yacc.

\subsection{Generacja kodu tabel}\label{subsec:generacja-kodu-tabel}

\subsubsection{Implementacja ToExpr dla złożonych struktur}\label{subsubsec:implementacja-toexpr-dla-zlozonych-struktur}

System wymaga konwersji struktur danych w czasie kompilacji (wartości) na kod (wyrażenia \texttt{Expr[T]}).
Realizowane jest to poprzez implementację instancji \texttt{ToExpr} dla typów \texttt{ParseTable} i \texttt{ActionTable}.

Implementacja \texttt{ToExpr[ParseTable]} jest szczególnie interesująca, gdyż musi radzić sobie z~potencjalnie dużymi tabelami (patrz \ref{subsubsec:problem-ograniczenia-rozmiaru-metod-jvm}):

\lstinputlisting[language=scala,caption={Implementacja ToExpr dla ParseTable},label={lst:parser-09-toexpr}, linerange={161-194}]{../../src/alpaca/parser/ParseTable.scala}

Ta implementacja demonstruje zaawansowane techniki metaprogramowania:
\begin{itemize}
  \item Tworzenie nowych symboli (\texttt{Symbol.newVal}) reprezentujących zmienne w generowanym kodzie
  \item Konstrukcja definicji wartości (\texttt{ValDef}) z przypisaniem początkowym
  \item Generacja listy wyrażeń manipulujących builderem
  \item Składanie wszystkiego w blok kodu (\texttt{Block}) z finalnym wynikiem
\end{itemize}

\subsection{Podsumowanie implementacji parsera}\label{subsec:podsumowanie-implementacji-parsera}

Implementacja generatora parserów w systemie ALPACA demonstruje zaawansowane zastosowanie metaprogramowania Scali 3 do rozwiązywania rzeczywistych problemów inżynierskich.
Kluczowe osiągnięcia obejmują:

\begin{itemize}
  \item \textbf{Automatyczna konstrukcja tabel LR(1)} w czasie kompilacji eliminująca potrzebę osobnego kroku generacji kodu
  \item \textbf{Pełna integracja z systemem typów Scali}, zapewniająca bezpieczeństwo typów w akcjach semantycznych
  \item \textbf{Eleganckie obejście ograniczeń platformy JVM} poprzez inteligentną fragmentację generowanego kodu
  \item \textbf{Deklaratywny mechanizm rozwiązywania konfliktów} oferujący większą elastyczność niż tradycyjne narzędzia
  \item \textbf{Natychmiastowa walidacja gramatyk} podczas kompilacji, z czytelnymi komunikatami o~błędach
\end{itemize}

Problemy napotkane i rozwiązane podczas implementacji, szczególnie ograniczenie rozmiaru metod JVM, ilustrują istotne aspekty praktycznego metaprogramowania: generowany kod musi nie tylko być poprawny funkcjonalnie, ale również respektować wszystkie techniczne ograniczenia platformy docelowej.

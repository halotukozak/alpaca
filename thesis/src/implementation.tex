\chapter{Implementacja}
\label{ch:implementacja}


\section{Praktyczna implementacja analizatora leksykalnego z wykorzystaniem makr w Scali 3}\label{sec:praktyczna-implementacja-analizatora-leksykalnego-z-wykorzystaniem-makr-w-scali-3}

\subsection{Wprowadzenie do studium przypadku}\label{subsec:wprowadzenie-do-studium-przypadku}

Niniejszy rozdział prezentuje praktyczną implementację systemu analizy leksykalnej (leksera) wykorzystującego zaawansowane mechanizmy metaprogramowania Scali 3.
Przedstawiony kod stanowi przykład zastosowania technik opisanych w poprzednim rozdziale do rozwiązania rzeczywistego problemu inżynierskiego: automatycznej generacji wydajnego analizatora leksykalnego z definicji wysokopoziomowej w formie języka dziedzinowego (DSL).

System \texttt{alpaca.lexer} implementuje transformację deklaratywnych reguł tokenizacji zapisanych jako funkcja częściowa (ang. \textit{partial function}) w kod procedualny wykonywany w czasie kompilacji.
Wykorzystuje przy tym pełne spektrum możliwości refleksji TASTy\cite{scala3-reflection}, włączając generację klas w czasie kompilacji, transformację drzew AST\cite{scala3-guides-reflection} oraz wyspecjalizowane typy refinement.

\subsection{Architektura systemu leksera}\label{subsec:architektura-systemu-leksera}

\subsubsection{Interfejs użytkownika}\label{subsubsec:interfejs-uzytkownika}

System oferuje użytkownikowi przejrzysty interfejs DSL oparty na dopasowaniu wzorców:

\lstinputlisting[language=scala,caption={Definicja typu LexerDefinition},label={lst:lexer-definition}]{listings/implementation/01-lexer-definition.scala}

Definicja \texttt{LexerDefinition} reprezentuje reguły leksera jako funkcję częściową mapującą wzorce wyrażeń regularnych (jako ciągi znaków) na definicje tokenów.
Wykorzystanie funkcji częściowej pozwala na naturalne wyrażenie reguł leksykalnych w idiomatycznej składni Scali.

Główny punkt wejścia systemu stanowi metoda \texttt{lexer}:

\lstinputlisting[language=scala,caption={Punkt wejścia: transparent inline def lexer},label={lst:lexer-entrypoint}]{listings/implementation/02-lexer-entrypoint.scala}

Modyfikator \texttt{transparent inline} zapewnia, że zwracany typ będzie dokładnie odpowiadał wygenerowanej strukturze, włączając typy refinement dla poszczególnych tokenów.
Użycie parametrów kontekstowych (\texttt{using}) realizuje wzorzec dependency injection na poziomie systemu typów.

\subsubsection{Implementacja makra}\label{subsubsec:implementacja-makra}

Makro przyjmuje wyrażenie reprezentujące reguły leksera jako \texttt{Expr[Ctx ?=> LexerDefinition[Ctx]]} oraz instancje kontekstualnych klas pomocniczych.
Parametr \texttt{using Quotes} dostarcza dostępu do API refleksji TASTy\cite{stucki2020inlining,scala3-guides-quotes,scala3-reference-macros}.

\subsection{Analiza drzewa składni abstrakcyjnej}\label{subsec:analiza-drzewa-skladni-abstrakcyjnej}

\subsubsection{Dekonstrukcja funkcji częściowej}\label{subsubsec:dekonstrukcja-funkcji-czesciowej}

Kluczowym krokiem implementacji jest ekstrakcja reguł z definicji funkcji częściowej:

\lstinputlisting[language=scala,caption={Dekonstrukcja funkcji częściowej (dopasowanie AST do CaseDef)},label={lst:extract-cases}]{listings/implementation/03-extract-case.scala}

Ten fragment kodu wykorzystuje dopasowanie wzorców w \textit{quotes} do dekonstrukcji\cite{scala3-guides-quotes} typowanego AST funkcji częściowej.
Struktura \texttt{Lambda(\_, Match(\_, cases))} odpowiada wewnętrznej reprezentacji funkcji częściowej, gdzie \texttt{Match} zawiera listę przypadków \texttt{CaseDef}.

\subsection{Transformacja i adaptacja referencji}\label{subsec:transformacja-i-adaptacja-referencji}

\subsubsection{Klasa replacerefs}\label{subsubsec:klasa-replacerefs}

Kluczową techniką jest zastąpienie referencji do starego kontekstu nowymi referencjami:

\lstinputlisting[language=scala,caption={Zastąpienie referencji starego kontekstu nowymi (ReplaceRefs)},label={lst:replace-with-new-ctx}]{listings/implementation/04-replace-refs-with-new-ctx.scala}

Transformacja ta realizuje proces znany jako ``re-owning'' w terminologii kompilatorów — zmianę właściciela (owner) symboli w AST. Jest to konieczne, ponieważ kod oryginalnie odnoszący się do parametru makra musi zostać przepisany, aby odnosił się do parametru metody w wygenerowanej klasie.
Klasa \texttt{ReplaceRefs} udostępnia \texttt{TreeMap}, który podczas przejścia po AST podmienia referencje do wskazanych symboli na podane termy\cite{scala3-guides-reflection}.

\subsection{Ekstrakcja i kompilacja wzorców}\label{subsec:ekstrakcja-i-kompilacja-wzorcow}

\subsubsection{Funkcja extractSimple}\label{subsubsec:funkcja-extractsimple}

Funkcja \texttt{extractSimple} implementuje logikę dopasowania różnych typów definicji tokenów:

\lstinputlisting[language=scala,caption={Funkcja extractSimple: dopasowywanie definicji tokenów},label={lst:lexer-08-extract-simple}]{listings/implementation/05-extract-simple.scala}

Wykorzystuje ona dopasowanie wzorców w \textit{quotes} z ekstraktorem typów\cite{scala3-guides-quotes}, umożliwiając rozróżnienie różnych wariantów definicji tokenów na poziomie typów.
Konstrukcja \texttt{type t <: ValidName} w wzorcu wiąże parametr typu do zmiennej wzorca \texttt{t}, umożliwiając jego późniejsze wykorzystanie.

\subsection{Analiza wzorców: klasa CompileNameAndPattern}
\label{subsec:compile-name-pattern}

Klasa \texttt{CompileNameAndPattern} stanowi kluczowy komponent systemu analizy leksykalnej, odpowiedzialny za ekstrakcję i walidację wzorców tokenów podczas ekspansji makra\cite{scala3-reference-macros}.
Jej głównym zadaniem jest transformacja różnorodnych form wzorców występujących w definicjach DSL na ujednolicone struktury \texttt{TokenInfo}, które następnie są wykorzystywane do generacji finalnego kodu leksera.

Implementacja wykorzystuje rekurencyjne przetwarzanie drzewa AST z zastosowaniem optymalizacji rekurencji ogonowej (\texttt{@tailrec}), co zapewnia efektywność działania nawet dla złożonych wzorców z wieloma alternatywami.

\subsection{Generacja klasy anonimowej}\label{subsec:generacja-klasy-anonimowej}

Kluczowym mechanizmem implementacyjnym makra \texttt{lexer} jest programatyczna konstrukcja klasy anonimowej w~czasie kompilacji\cite{stucki2021multistage}.
Proces ten wykorzystuje API refleksji TASTy\cite{scala3-reflection} do dynamicznego tworzenia struktur typów, które następnie są materializowane jako kod bajtowy JVM.

\subsubsection{Konstrukcja symbolu klasy}\label{subsubsec:konstrukcja-symbolu-klasy}

Anonimowa klasa implementująca \texttt{Tokenization[Ctx]} jest tworzona poprzez wywołanie \texttt{Symbol.newClass}:

Metoda \texttt{Symbol.newClass} przyjmuje następujące parametry:
\begin{itemize}
  \item \textbf{Symbol.spliceOwner} — właściciel nowego symbolu w~hierarchii definiowania, zapewniający poprawną widoczność w~zakresie leksykalnym
  \item \textbf{Symbol.freshName("\$anon")} — generowanie unikalnej nazwy klasy zgodnie z~konwencją kompilatora Scali dla klas anonimowych
  \item \textbf{List(TypeRepr.of[Tokenization[Ctx]])} — lista typów bazowych, w~tym przypadku pojedyncza implementacja abstrakcyjnej klasy \texttt{Tokenization}
  \item \textbf{decls} — funkcja dostarczająca listy deklaracji członków klasy (pól i~metod)
\end{itemize}

\subsubsection{Definicja członków klasy}\label{subsubsec:definicja-czlonkow-klasy}

Funkcja \texttt{decls} konstruuje pełną listę deklaracji dla klasy anonimowej:

\begin{enumerate}
  \item \textbf{Pola tokenów} — dla każdego zdefiniowanego tokena tworzony jest symbol pola typu \texttt{DefinedToken[Name, Ctx, Value]}
  \item \textbf{Type alias Fields} — typ pomocniczy w~formie \texttt{NamedTuple} ułatwiający strukturalny dostęp do tokenów
  \item \textbf{Pole compiled} — wartość typu \texttt{Regex} zawierająca skompilowane wyrażenie regularne dla wszystkich tokenów
  \item \textbf{Pole tokens} — lista wszystkich zdefiniowanych tokenów (włączając ignorowane)
  \item \textbf{Pole byName} — mapa umożliwiająca dynamiczny dostęp do tokenów po nazwie
\end{enumerate}

\subsubsection{Materializacja klasy}\label{subsubsec:materializacja-klasy}

Po zdefiniowaniu symbolu klasy następuje konstrukcja jej ciała.
Klasa jest następnie instancjonowana poprzez wywołanie jej konstruktora.

\subsection{Typy rafinowane (refinement types)}\label{subsec:typy-rafinowane}

Mechanizm typów rafinowanych stanowi fundamentalną cechę systemu typów Scali umożliwiającą precyzyjne wyrażenie struktury typów w~czasie kompilacji\cite{scala3-selectable}.
W~kontekście implementacji leksera typy rafinowane pozwalają na dodanie informacji o~polach tokenów bezpośrednio do typu zwracanego przez makro.

\subsubsection{Proces rafinowania typu}\label{subsubsec:proces-rafinowania-typu}

Typ wynikowy jest konstruowany poprzez iteracyjne rafinowanie typu bazowego\cite{scala3-guides-reflection}:


\lstinputlisting[language=scala,caption={Rafinowanie typu wynikowego o pola tokenów},label={lst:refinements}]{listings/implementation/06-refinements.scala}

Funkcja \texttt{Refinement(tpe, name, memberType)} tworzy nowy typ będący rozszerzeniem typu.
Operacja ta jest wykonywana w~czasie kompilacji i~nie generuje dodatkowego kodu w~czasie wykonania.

\subsubsection{Wynikowy typ}\label{subsubsec:wynikowy-typ}

Wynikowy typ ma formę typu przecięcia (ang. \textit{intersection type}):

\begin{lstlisting}[language=scala,caption={Wynikowy typ leksera},label={lst:lexer-15-result-type}]
Tokenization[Ctx] & { 
  val TOKEN1: DefinedToken["NAME1", Ctx, Type1]
  val TOKEN2: DefinedToken["NAME2", Ctx, Type2]
  ...
}
\end{lstlisting}

Ten typ reprezentuje wartości będące jednocześnie instancjami \texttt{Tokenization[Ctx]} oraz posiadające określone pola strukturalne (ang. \emph{computed field names}).

Dostęp do pól tokenów odbywa się poprzez \verb|trait Selectable|, który jest implementowany przez klasę generowaną przez makro. Mechanizm działania typów strukturalnych został szczegółowo opisany w dokumentacji Scali \cite{scala3-selectable}.
Aby mechanizm \texttt{Selectable} działał poprawnie ze strukturalnymi typami i nie wymagał refleksji, klasa generowana przez makro musi implementować \verb|type Fields <: NamedTuple.AnyNamedTuple|\cite{scala3-computed-field-names}.
W naszym podejściu makro generuje definicję \texttt{type Fields} zawierającą wszystkie zdefiniowane tokeny i ich typy, dzięki czemu:
\begin{itemize}
  \item IDE oraz kompilator znają z góry dostępne pola i ich typy (pełne uzupełnianie i sprawdzanie typów),
  \item wywołanie \texttt{c.NAZWA} jest bezpieczne typowo mimo mechanizmu dynamicznego wyboru nazwy.
\end{itemize}


\lstinputlisting[language=scala,caption={Tworzenie typuFields},label={lst:fields}]{listings/implementation/07-fields.scala}


\subsection{Uzasadnienie wybranego podejścia implementacyjnego}\label{subsec:uzasadnienie-wybranego-podejscia}

\subsubsection{Eliminacja narzutu wykonania w~czasie działania programu}\label{subsubsec:eliminacja-narzutu-wykonania}

Wszystkie definicje tokenów są rozwiązywane statycznie w~czasie kompilacji\cite{stucki2020inlining}.
Dostęp do tokenów odbywa się poprzez bezpośrednie odwołanie do pola klasy, co po kompilacji do kodu bajtowego JVM\cite{lindholm2014java} redukuje się do instrukcji \texttt{getfield} — operacji o~złożoności O(1) bez żadnego narzutu pośrednictwa.

Alternatywne podejście oparte na strukturze mapującej (np.\ \texttt{Map[String, Token]}) wymagałoby:
\begin{itemize}
  \item Obliczenia funkcji haszującej dla klucza
  \item Przeszukiwania tablicy haszującej
  \item Potencjalnej obsługi kolizji
  \item Dynamicznego rzutowania typu
\end{itemize}

co wprowadzałoby znaczący narzut wydajnościowy oraz eliminowało możliwość optymalizacji przez kompilator.

\subsubsection{Bezpieczeństwo typów na poziomie systemu}\label{subsubsec:bezpieczenstwo-typow}

Dzięki typom rafinowanym każdy token posiada precyzyjny typ znany kompilatorowi\cite{scala3-selectable}.
System typów weryfikuje poprawność wszystkich operacji w~czasie kompilacji, eliminując możliwość błędów związanych z~niepoprawnym typowaniem wartości tokenów.

\subsubsection{Integracja z~narzędziami deweloperskimi}\label{subsubsec:integracja-z-narzedzimi}

Ponieważ tokeny są reprezentowane jako rzeczywiste pola w~typie, środowiska deweloperskie (IDE) mogą wykorzystać informacje typu do:
\begin{itemize}
  \item Automatycznego uzupełniania nazw tokenów
  \item Prezentacji pełnych sygnatur typów przy najechaniu kursorem
  \item Nawigacji do definicji przez mechanizm \textit{go-to-definition}
  \item Wykrywania błędów składniowych przed kompilacją
\end{itemize}

Te funkcjonalności są niemożliwe do realizacji w~przypadku dostępu przez struktury dynamiczne.

\subsubsection{Statyczna detekcja konfliktów wzorców}\label{subsubsec:statyczna-detekcja-konfliktow}

Makro przeprowadza analizę wszystkich wzorców w~czasie kompilacji, wykrywając potencjalne konflikty nakładających się wyrażeń regularnych.
Mechanizm ten zapewnia, że błędy konfiguracji są wykrywane na etapie kompilacji, a~nie w~czasie wykonania programu, co jest zgodne z~zasadą \textit{fail-fast} w~inżynierii oprogramowania.

\subsubsection{Typowanie strukturalne z~gwarancjami nominalnymi}\label{subsubsec:typowanie-strukturalne}

Zastosowanie typów rafinowanych\cite{scala3-selectable} łączy zalety typowania strukturalnego (elastyczność w~dostępie do składowych) z~bezpieczeństwem typowania nominalnego (jednoznaczna identyfikacja typów).
Każde pole w~typie rafinowanym ma precyzyjny typ nominalny, podczas gdy dostęp do tych pól odbywa się przez nazwę, co zapewnia elastyczność interfejsu.

\subsection{Analiza alternatywnych rozwiązań}\label{subsec:analiza-alternatywnych-rozwiazan}

\subsubsection{Podejście oparte na mapowaniu dynamicznym}\label{subsubsec:podejscie-mapowanie-dynamiczne}

Alternatywne podejście mogłoby wykorzystywać strukturę mapującą do przechowywania tokenów:

\begin{lstlisting}[language=scala,caption={Podejście oparte na mapowaniu dynamicznym}]
class SimpleLexer {
  val tokens: Map[String, Token[?, ?, ?]] = Map(
    "NUMBER" -> ...,
    "PLUS" -> ...
  )
  def apply(name: String): Token[?, ?, ?] = tokens(name)
}
\end{lstlisting}

\textbf{Wady tego podejścia:}
\begin{itemize}
  \item Brak bezpieczeństwa typów: błędne nazwy tokenów wykrywane są dopiero w~czasie wykonania
  \item Utrata informacji o~typach: zwracany typ to egzystencjalny \texttt{Token[?, ?, ?]}
  \item Narzut wydajnościowy operacji haszowania i~przeszukiwania
  \item Brak wsparcia narzędzi deweloperskich
\end{itemize}

\subsubsection{Podejście oparte na jawnej definicji klasy}\label{subsubsec:podejscie-jawna-definicja}

Innym rozwiązaniem byłoby jawne definiowanie klasy leksera przez użytkownika:

\begin{lstlisting}[language=scala,caption={Podejście oparte na jawnej definicji klasy}]
class MyLexer extends Tokenization[DefaultGlobalCtx] {
  val NUMBER = DefinedToken[...]
  val PLUS = DefinedToken[...]
  protected def compiled: Regex = "(?<token0>[0-9]+)|(?<token1>\\+)".r
  // ...
}
\end{lstlisting}

\textbf{Wady tego podejścia:}
\begin{itemize}
  \item Wysoki poziom redundancji kodu (\textit{boilerplate})
  \item Konieczność ręcznej kompilacji wyrażeń regularnych
  \item Podatność na błędy synchronizacji między definicjami tokenów a~wyrażeniem regularnym
  \item Brak mechanizmu DSL ułatwiającego definicję reguł
\end{itemize}

\subsection{Walidacja i obsługa błędów}\label{subsec:walidacja-i-obsuga-bedow}

\subsubsection{Walidacja wzorców regularnych}\label{subsubsec:walidacja-wzorcow-regularnych}

System wykorzystuje pomocniczą klasę \texttt{RegexChecker} do walidacji wzorców:
Mechanizm ten sprawdza poprawność składni wyrażeń regularnych już w czasie kompilacji i raportuje błędy z dokładną lokalizacją wzorca.
Metoda \texttt{report.errorAndAbort} jest częścią API kompilatora do raportowania błędów w czasie kompilacji\cite{scala3-reference-macros,scala3-guides-macros}.
Przerwanie kompilacji w przypadku niepoprawnych wzorców zapewnia, że błędy konfiguracji są wykrywane możliwie wcześnie.

\subsubsection{Obsługa nieobsługiwanych konstrukcji}\label{subsubsec:obsuga-nieobsugiwanych-konstrukcji}

Kod jawnie sygnalizuje nieobsługiwane przypadki:
Obsługiwane są wyłącznie jasno zdefiniowane formy wzorców; w przypadku napotkania innej konstrukcji kompilacja jest przerywana z komunikatem zawierającym szczegóły AST, co upraszcza diagnostykę i utrzymuje zasadę fail-fast.
Ta strategia jest zgodna z zasadą fail-fast - lepiej jest wyraźnie odrzucić nieobsługiwane konstrukcje niż milcząco generować niepoprawny kod.


\section{Praktyczna implementacja analizatora leksykalnego z wykorzystaniem makr w Scali 3}\label{ch:praktyczna-implementacja-analizatora-leksykalnego-z-wykorzystaniem-makr-w-scali-3}

\subsection{Wprowadzenie do studium przypadku}\label{subsec:wprowadzenie-do-studium-przypadku}

Niniejszy rozdział prezentuje praktyczną implementację systemu analizy leksykalnej (leksera) wykorzystującego zaawansowane mechanizmy metaprogramowania Scali 3.
Przedstawiony kod stanowi przykład zastosowania technik opisanych w poprzednich rozdziałach do rozwiązania rzeczywistego problemu inżynierskiego: automatycznej generacji wydajnego analizatora leksykalnego z definicji wysokopoziomowej w formie języka dziedzinowego (DSL).

System \texttt{alpaca.lexer} implementuje transformację deklaratywnych reguł tokenizacji zapisanych jako funkcja częściowa (partial function) w kod procedualny wykonywany w czasie kompilacji.
Wykorzystuje przy tym pełne spektrum możliwości refleksji TASTy, włączając generację klas w czasie kompilacji, transformację drzew AST oraz wyspecjalizowane typy refinement.

\subsection{Architektura systemu leksera}\label{subsec:architektura-systemu-leksera}

\subsubsection{Interfejs użytkownika}\label{subsubsec:interfejs-uzytkownika}

System oferuje użytkownikowi przejrzysty interfejs DSL oparty na dopasowaniu wzorców:

\lstinputlisting[language=scala,caption={Definicja typu LexerDefinition},label={lst:lexer-01-lexerdefinition}, linerange={21}]{../../src/alpaca/lexer/Lexer.scala}

Definicja \texttt{LexerDefinition} reprezentuje reguły leksera jako funkcję częściową mapującą wzorce wyrażeń regularnych (jako ciągi znaków) na definicje tokenów.
Wykorzystanie funkcji częściowej pozwala na naturalne wyrażenie reguł leksykalnych w idiomatycznej składni Scali.

Główny punkt wejścia systemu stanowi metoda \texttt{inline}:

\lstinputlisting[language=scala,caption={Punkt wejścia: transparent inline def lexer},label={lst:lexer-02-entrypoint}, linerange={44-52}]{../../src/alpaca/lexer/Lexer.scala}

Modyfikator \texttt{transparent inline} zapewnia, że zwracany typ będzie dokładnie odpowiadał wygenerowanej strukturze, włączając typy refinement dla poszczególnych tokenów.
Użycie parametrów kontekstowych (\texttt{using}) realizuje wzorzec dependency injection na poziomie systemu typów.

\subsubsection{Implementacja makra}\label{subsubsec:implementacja-makra}

Implementacja makra \texttt{lexerImpl} stanowi serce systemu:

\lstinputlisting[language=scala,caption={Implementacja makra: podpis lexerImpl},label={lst:lexer-03-lexerimpl-signature}, linerange={54-59}]{../../src/alpaca/lexer/Lexer.scala}

Makro przyjmuje wyrażenie reprezentujące reguły leksera jako \texttt{Expr[Ctx ?=> LexerDefinition[Ctx]]} oraz instancje kontekstualnych klas pomocniczych.
Parametr \texttt{using Quotes} dostarcza dostępu do API refleksji TASTy.

\subsection{Analiza drzewa składni abstrakcyjnej}\label{subsec:analiza-drzewa-skadni-abstrakcyjnej}

\subsubsection{Dekonstrukcja funkcji częściowej}\label{subsubsec:dekonstrukcja-funkcji-czesciowej}

Kluczowym krokiem implementacji jest ekstrakcja reguł z definicji funkcji częściowej:

\lstinputlisting[language=scala,caption={Dekonstrukcja funkcji częściowej (dopasowanie AST do CaseDef)},label={lst:lexer-04-extract-cases}, linerange={72}]{../../src/alpaca/lexer/Lexer.scala}

Ten fragment kodu wykorzystuje dopasowanie wzorców w cytatach (quote pattern matching) do dekonstrukcji typowanego AST funkcji częściowej.
Struktura \texttt{Lambda(\_, Match(\_, cases))} odpowiada wewnętrznej reprezentacji funkcji częściowej, gdzie \texttt{Match} zawiera listę przypadków \texttt{CaseDef}.

Metoda \texttt{asTerm} konwertuje \texttt{Expr[T]} na \texttt{Term}, umożliwiając bezpośrednią manipulację AST. Pole \texttt{underlying} dostarcza dostęp do wewnętrznej reprezentacji, a \texttt{runtimeChecked} zapewnia walidację w czasie wykonania makra.

\subsubsection{Przetwarzanie przypadków dopasowania}\label{subsubsec:przetwarzanie-przypadkow-dopasowania}

Każdy przypadek (\texttt{CaseDef}) jest przetwarzany indywidualnie w operacji fold:

\lstinputlisting[language=scala,caption={Fold po przypadkach dopasowania do zbudowania listy tokenów i metadanych},label={lst:lexer-05-cases-fold}, linerange={73,74}]{../../src/alpaca/lexer/Lexer.scala}

Struktura \texttt{CaseDef(tree, None, body)} dekonstruuje przypadek dopasowania na wzorzec (\texttt{tree}), opcjonalny guard (\texttt{None} w tym przypadku) oraz ciało (\texttt{body}). System obecnie nie wspiera przypadków z guardami, co jest jawnie zasygnalizowane w kodzie.

\subsection{Transformacja i adaptacja referencji}\label{subsec:transformacja-i-adaptacja-referencji}

\subsubsection{Klasa replacerefs}\label{subsubsec:klasa-replacerefs}

Kluczową techniką jest zastąpienie referencji do starego kontekstu nowymi referencjami:

\lstinputlisting[language=scala,caption={Zastąpienie referencji starego kontekstu nowymi (ReplaceRefs)},label={lst:lexer-06-replace-with-new-ctx}, linerange={75-78}]{../../src/alpaca/lexer/Lexer.scala}

Transformacja ta realizuje proces znany jako \("\)re-owning\("\) w terminologii kompilatorów — zmianę właściciela (owner) symboli w AST. Jest to konieczne, ponieważ kod oryginalnie odnoszący się do parametru makra musi zostać przepisany, aby odnosił się do parametru metody w wygenerowanej klasie.

Konstrukcja \texttt{Select.unique(newCtx, "lastRawMatched")} generuje wyrażenie dostępu do pola obiektu, odpowiadające składni \texttt{newCtx.lastRawMatched} w kodzie Scali.

\subsubsection{Funkcja CreateLambda}\label{subsubsec:funkcja-createlambda}

Klasa \texttt{CreateLambda} służy do generowania wyrażeń lambda w AST:

\lstinputlisting[language=scala,caption={Generowanie lambdy z re-owningiem ownerów},label={lst:lexer-07-create-lambda}, linerange={115-119}]{../../src/alpaca/lexer/Lexer.scala}

Metoda \texttt{changeOwner} modyfikuje symbol właściciela dla wszystkich węzłów w podrzewie AST. Jest to kluczowe dla zapewnienia, że wygenerowany kod będzie poprawnie typowany i możliwy do wykonania.

\subsection{Ekstrakcja i kompilacja wzorców}\label{subsec:ekstrakcja-i-kompilacja-wzorcow}

\subsubsection{Funkcja extractSimple}\label{subsubsec:funkcja-extractsimple}

Funkcja \texttt{extractSimple} implementuje logikę dopasowania różnych typów definicji tokenów:

\lstinputlisting[language=scala,caption={Funkcja extractSimple: dopasowywanie definicji tokenów},label={lst:lexer-08-extract-simple}, linerange={80-85,99-108}]{../../src/alpaca/lexer/Lexer.scala}

Wykorzystuje ona dopasowanie wzorców w cytatach z ekstraktorem typów, umożliwiając rozróżnienie różnych wariantów definicji tokenów na poziomie typów.
Konstrukcja \texttt{type t <: ValidName} w wzorcu wiąże parametr typu do zmiennej wzorca \texttt{t}, umożliwiając jego późniejsze wykorzystanie.

\subsubsection{compilenameandpattern}\label{subsubsec:compilenameandpattern}

Pomocnicza klasa \texttt{CompileNameAndPattern} jest odpowiedzialna za walidację i kompilację wzorców wyrażeń regularnych:


\section{Analiza wzorców: klasa CompileNameAndPattern}
\label{sec:compile-name-pattern}

\subsection{Wprowadzenie}
\label{subsec:cnp-intro}

Klasa \texttt{CompileNameAndPattern} stanowi kluczowy komponent systemu analizy leksykalnej, odpowiedzialny za ekstrakcję i walidację wzorców tokenów podczas ekspansji makra. Jej głównym zadaniem jest transformacja różnorodnych form wzorców występujących w definicjach DSL na ujednolicone struktury \texttt{TokenInfo}, które następnie są wykorzystywane do generacji finalnego kodu leksera.

Implementacja wykorzystuje rekurencyjne przetwarzanie drzewa AST z zastosowaniem optymalizacji rekurencji ogonowej (\texttt{@tailrec}), co zapewnia efektywność działania nawet dla złożonych wzorców z wieloma alternatywami.

\subsection{Architektura klasy}
\label{subsec:cnp-architecture}

Klasa jest sparametryzowana typem \texttt{Quotes}, co pozwala na pełną integrację z API refleksji TASTy:

\lstinputlisting[language=scala,caption={Architektura klasy CompileNameAndPattern: sygnatura i pola},label={lst:cnp-01-architecture}, linerange={20-21,33}]{../../src/alpaca/lexer/CompileNameAndPattern.scala}

Parametr typu \texttt{T: Type} reprezentuje typ wzorca w systemie typów Scali 3, który jest analizowany w celu określenia nazwy tokena. Wzorzec \texttt{pattern: Tree} to fragment drzewa AST reprezentujący lewą stronę przypadku dopasowania w definicji leksera.

\subsection{Algorytm rekurencyjnego przetwarzania}
\label{subsec:cnp-algorithm}

Główna metoda \texttt{apply} deleguje przetwarzanie do wewnętrznej funkcji rekurencyjnej \texttt{loop}:

Funkcja \texttt{loop} przyjmuje:
\begin{itemize}
    \item \texttt{tpe: TypeRepr} --- reprezentację typu wzorca, wykorzystywaną do określenia nazwy tokena
    \item \texttt{pattern: Tree} --- drzewo AST wzorca, z którego ekstrahowany jest wzorzec wyrażenia regularnego
\end{itemize}

Adnotacja \texttt{@tailrec} zapewnia, że kompilator zweryfikuje poprawność optymalizacji rekurencji ogonowej, co jest kluczowe dla wydajności przy przetwarzaniu złożonych wzorców.

\subsection{Przypadki dopasowania wzorców}
\label{subsec:cnp-cases}

\subsubsection{Wzorzec z wiązaniem i literałem}
\label{subsubsec:cnp-bind-literal}

Pierwszy przypadek obsługuje sytuację, gdy nazwa tokena jest wyprowadzana z nazwy zmiennej wiązania:

\lstinputlisting[language=scala,caption={Wiązanie i literał: wyprowadzenie nazwy tokena},label={lst:cnp-02-bind-literal}, linerange={37-38}]{../../src/alpaca/lexer/CompileNameAndPattern.scala}


\textbf{Struktura AST:} Ten przypadek odpowiada wzorcowi postaci:
\begin{lstlisting}
case x @ "\\d+" => Token[x.type]
\end{lstlisting}

gdzie:
\begin{itemize}
    \item \texttt{TermRef(qual, name)} --- reprezentacja typu tokena jako referencja do termu
    \item \texttt{Bind(bind, ...)} --- operacja wiązania zmiennej \texttt{x}
    \item \texttt{Literal(StringConstant(regex))} --- literał ciągu znaków zawierający wyrażenie regularne
    \item warunek \texttt{name == bind} --- weryfikacja, że nazwa typu odpowiada nazwie zmiennej
\end{itemize}

Rezultat: nazwa tokena i wzorzec są identyczne (oba równe \texttt{regex}).

\subsubsection{Wzorzec z wiązaniem i alternatywami}
\label{subsubsec:cnp-bind-alternatives}

Drugi przypadek rozszerza poprzedni o obsługę alternatyw:

\lstinputlisting[language=scala,caption={Wiązanie z alternatywami: generacja wielu TokenInfo},label={lst:cnp-03-bind-alternatives}, linerange={40-45}]{../../src/alpaca/lexer/CompileNameAndPattern.scala}

\textbf{Struktura AST:} Odpowiada wzorcowi:
\begin{lstlisting}
case x @ ("+" | "-" | "*" | "/") => Token[x.type]
\end{lstlisting}

gdzie \texttt{Alternatives(alternatives)} reprezentuje konstrukcję \texttt{("+" | "-" | ...)}. System generuje osobny \texttt{TokenInfo} dla każdej alternatywy, wszystkie z tą samą nazwą tokena (wyprowadzoną z \texttt{x}).

\subsubsection{Wzorzec ignorowany z literałem}
\label{subsubsec:cnp-ignored-literal}

Trzeci przypadek obsługuje tokeny ignorowane (bez przypisanej nazwy):

\lstinputlisting[language=scala,caption={Token ignorowany: literał regex},label={lst:cnp-04-ignored-literal}, linerange={50,51}]{../../src/alpaca/lexer/CompileNameAndPattern.scala}

\textbf{Struktura AST:} Odpowiada wzorcowi:
\begin{lstlisting}
case "\\s+" => Token.Ignored
\end{lstlisting}

Typ \texttt{Nothing} sygnalizuje, że token nie ma przypisanej nazwy w systemie typów. Zarówno nazwa jak i wzorzec są ustawiane na wartość literału.

\subsubsection{Wzorzec ignorowany z alternatywami}
\label{subsubsec:cnp-ignored-alternatives}

Czwarty przypadek łączy tokeny ignorowane z alternatywami:

\lstinputlisting[language=scala,caption={Token ignorowany: alternatywy regex},label={lst:cnp-05-ignored-alternatives}, linerange={53-57}]{../../src/alpaca/lexer/CompileNameAndPattern.scala}

\textbf{Struktura AST:} Odpowiada wzorcowi:
\begin{lstlisting}
case "\\s+" | "\\t+" | "\\n+" => Token.Ignored
\end{lstlisting}

\subsubsection{Wzorzec z typem stałym i literałem}
\label{subsubsec:cnp-constant-literal}

Piąty przypadek obsługuje jawne określenie nazwy tokena przez typ:

\lstinputlisting[language=scala,caption={Typ stały i literał: jawna nazwa tokena},label={lst:cnp-06-const-literal}, linerange={59-60}]{../../src/alpaca/lexer/CompileNameAndPattern.scala}

\textbf{Struktura AST:} Odpowiada wzorcowi:
\begin{lstlisting}
case "\\d+" => Token["number"]
\end{lstlisting}

gdzie \texttt{ConstantType(StringConstant(name))} reprezentuje typ literalny ciągu znaków \texttt{"number"}. W tym przypadku nazwa tokena (\texttt{name}) i wzorzec (\texttt{regex}) są rozdzielone.

\subsubsection{Wzorzec z typem stałym i alternatywami}
\label{subsubsec:cnp-constant-alternatives}

Szósty przypadek łączy jawną nazwę z alternatywnymi wzorcami:

\lstinputlisting[language=scala,caption={Typ stały i alternatywy: łączenie wzorców},label={lst:cnp-07-const-alternatives}, linerange={62-71}]{../../src/alpaca/lexer/CompileNameAndPattern.scala}


\textbf{Struktura AST:} Odpowiada wzorcowi:
\begin{lstlisting}
case ("+" | "-") => Token["operator"]
\end{lstlisting}

Wszystkie alternatywy są łączone w jeden wzorzec z operatorem \texttt{|}, a nazwa tokena jest wspólna dla wszystkich.

\subsubsection{Wzorzec z zagnieżdżonym wiązaniem}
\label{subsubsec:cnp-nested-bind}

\subsection{Walidacja i konstrukcja TokenInfo}
\label{subsec:cnp-validation}

\subsubsection{Walidacja nazwy tokena}
\label{subsubsec:cnp-name-validation}

Funkcja \texttt{validateName} zapewnia, że nazwy tokenów spełniają wymagania systemu:

\begin{lstlisting}
private def validateName(name: String)(using quotes: Quotes): ValidName =
  import quotes.reflect.*
  name match
    case invalid @ "_" =>
      report.errorAndAbort(s"Invalid token name: $invalid")
    case other => other
\end{lstlisting}

Nazwa \texttt{"\_"} jest zarezerwowana w Scali i nie może być użyta jako nazwa tokena. W przypadku wykrycia niepoprawnej nazwy, kompilacja jest przerywana z informacyjnym komunikatem błędu.

\subsubsection{Konstrukcja wyrażenia TokenInfo}
\label{subsubsec:cnp-tokeninfo-construction}

Obiekt \texttt{Result} zawiera metodę \texttt{unsafe}, która konstruuje wyrażenie \texttt{TokenInfo}:

\begin{lstlisting}
def unsafe(name: String, regex: String)(using quotes: Quotes): Expr[TokenInfo[?]] = {
  import quotes.reflect.*
  val validatedName = validateName(name)
  ConstantType(StringConstant(validatedName)).asType match
    case '[type nameTpe <: ValidName; nameTpe] =>
      '{ TokenInfo[nameTpe](
           ${ Expr(validatedName).asExprOf[nameTpe] },
           ${ Expr(regex) }
         ) }
}
\end{lstlisting}

Proces konstrukcji obejmuje:

\begin{enumerate}
    \item \textbf{Walidację nazwy:} wywołanie \texttt{validateName(name)}
    \item \textbf{Konwersję na typ:} \texttt{ConstantType(StringConstant(validatedName)).asType}
    \item \textbf{Wiązanie typu:} dopasowanie wzorcem typu \texttt{[type nameTpe <: ValidName; nameTpe]}
    \item \textbf{Konstrukcję wyrażenia:} cytowanie kodu tworzącego \texttt{TokenInfo}
    \item \textbf{Wstawki wartości:} \texttt{\$\{ Expr(validatedName).asExprOf[nameTpe] \}} i \texttt{\$\{ Expr(regex) \}}
\end{enumerate}

Kluczową techniką jest użycie \texttt{asExprOf[nameTpe]} do precyzyjnego określenia typu literału ciągu znaków jako typu singleton. To zapewnia, że system typów śledzi dokładną wartość nazwy tokena.

\subsection{Tabela translacji wzorców}
\label{subsec:cnp-translation-table}

Tabela \ref{tab:pattern-translation} przedstawia szczegółową translację wszystkich obsługiwanych form wzorców z DSL leksera na odpowiadające im struktury AST oraz wynikowe wartości \texttt{TokenInfo}.

\begin{table}[htbp]
    \centering
    \caption{Translacja wzorców DSL na struktury TokenInfo}
    \label{tab:pattern-translation}
    \small
    \begin{tabular}{|p{4cm}|p{4cm}|p{3cm}|p{2.5cm}|}
        \hline
        \textbf{Wzorzec DSL} & \textbf{Struktura AST} & \textbf{Nazwa tokena} & \textbf{Wzorzec regex} \\
        \hline
        \texttt{case x @ "\\textbackslash\textbackslash d+" => Token[x.type]} &
        \texttt{TermRef(qual, name), Bind(bind, Literal(...))} &
        \texttt{x} (z wiązania) &
        \texttt{"\\textbackslash\textbackslash d+"} \\
        \hline
        \texttt{case x @ ("+" | "-") => Token[x.type]} &
        \texttt{TermRef(...), Bind(bind, Alternatives(...))} &
        \texttt{x} (dla każdej alternatywy) &
        \texttt{"+"}, \texttt{"-"} (osobno) \\
        \hline
        \texttt{case "\\textbackslash\textbackslash s+" => Token.Ignored} &
        \texttt{TypeRepr.of[Nothing], Literal(StringConstant(str))} &
        \texttt{"\\textbackslash\textbackslash s+"} (wzorzec) &
        \texttt{"\\textbackslash\textbackslash s+"} \\
        \hline
        \texttt{case ("\\textbackslash\textbackslash s+" | "\\textbackslash\textbackslash t+") => Token.Ignored} &
        \texttt{TypeRepr.of[Nothing], Alternatives(...)} &
        Każdy wzorzec osobno &
        \texttt{"\\textbackslash\textbackslash s+"}, \texttt{"\\textbackslash\textbackslash t+"} (osobno) \\
        \hline
        \texttt{case "\\textbackslash\textbackslash d+" => Token["number"]} &
        \texttt{ConstantType(StringConstant("number")), Literal(...)} &
        \texttt{"number"} (z typu) &
        \texttt{"\\textbackslash\textbackslash d+"} \\
        \hline
        \texttt{case ("+" | "-") => Token["operator"]} &
        \texttt{ConstantType(...), Alternatives(...)} &
        \texttt{"operator"} (z typu) &
        \texttt{"+" | "-"} (połączone) \\
        \hline
        \texttt{case x @ y @ "\\textbackslash\textbackslash w+" => Token[x.type]} &
        \texttt{TermRef(...), Bind(\_, Bind(\_, Literal(...)))} &
        \texttt{x} (zewnętrzne wiązanie) &
        \texttt{"\\textbackslash\textbackslash w+"} (po rekurencji) \\
        \hline
    \end{tabular}
\end{table}

\subsection{Semantyka alternatyw}
\label{subsec:cnp-alternatives-semantics}

Kluczową decyzją projektową jest sposób obsługi alternatyw w różnych kontekstach:

\begin{itemize}
    \item \textbf{Alternatywy z wiązaniem typu:} \texttt{case x @ ("a" | "b") => Token[x.type]}

    Generuje \textit{osobne} \texttt{TokenInfo} dla każdej alternatywy, wszystkie z nazwą \texttt{x}. To pozwala na indywidualne śledzenie, który dokładnie wzorzec został dopasowany.

    \item \textbf{Alternatywy z jawną nazwą:} \texttt{case ("a" | "b") => Token["token"]}

    Generuje \textit{jeden} \texttt{TokenInfo} z nazwą \texttt{"token"} i wzorcem \texttt{"a|b"}. To optymalizacja, ponieważ nie ma potrzeby rozróżniania alternatyw.
\end{itemize}

Ta różnica jest wyrażona matematycznie:

\begin{equation}
    \text{Bind}(x, \text{Alt}(a_1, \ldots, a_n)) \mapsto [\text{TokenInfo}(x, a_1), \ldots, \text{TokenInfo}(x, a_n)]
\end{equation}

\begin{equation}
    \text{Const}(c, \text{Alt}(a_1, \ldots, a_n)) \mapsto [\text{TokenInfo}(c, a_1 | \cdots | a_n)]
\end{equation}

\subsection{Obsługa błędów i nieobsługiwanych przypadków}
\label{subsec:cnp-error-handling}

System wykorzystuje funkcję \texttt{raiseShouldNeverBeCalled} do sygnalizacji nieoczekiwanych struktur AST:

\begin{lstlisting}
case x =>
  raiseShouldNeverBeCalled(x.toString)
\end{lstlisting}

Ta strategia jest uzasadniona następującymi argumentami:

\begin{enumerate}
    \item \textbf{Kompletność typu:} Kompilator Scali 3 gwarantuje, że przypadki dopasowania są wyczerpujące dla danego typu. Jeśli kod osiąga domyślny przypadek, oznacza to błąd w logice makra, nie błąd użytkownika.

    \item \textbf{Debugging:} Wywołanie \texttt{raiseShouldNeverBeCalled(x.toString)} dostarcza pełną reprezentację nieoczekiwanej struktury AST, co znacznie ułatwia diagnozowanie problemów.

    \item \textbf{Fail-fast:} Natychmiastowe przerwanie kompilacji zapobiega propagacji błędów i generacji niepoprawnego kodu.
\end{enumerate}

\subsection{Integracja z systemem typów}
\label{subsec:cnp-type-integration}

Klasa \texttt{CompileNameAndPattern} demonstruje zaawansowane wykorzystanie systemu typów Scali 3:

\subsubsection{Typy singleton dla nazw tokenów}
\label{subsubsec:cnp-singleton-types}

Wykorzystanie typów stałych (\texttt{ConstantType}) pozwala na śledzenie dokładnych wartości nazw tokenów na poziomie typów:

\begin{lstlisting}
ConstantType(StringConstant(validatedName)).asType match
  case '[type nameTpe <: ValidName; nameTpe] => (*$\ldots$*)
\end{lstlisting}

Jeśli \texttt{validatedName = "number"}, typ \texttt{nameTpe} będzie równoważny typowi literalnemu \texttt{"number"}, co pozwala kompilatorowi śledzić tę konkretną wartość.

\subsubsection{Context bounds i Type Evidence}
\label{subsubsec:cnp-context-bounds}

Metoda \texttt{apply} wykorzystuje context bound \texttt{T: Type}:

\begin{lstlisting}
def apply[T: Type](pattern: Tree): List[Expr[TokenInfo[?]]]
\end{lstlisting}

Jest to syntaktyczny skrót dla:

\begin{lstlisting}
def apply[T](pattern: Tree)(using Type[T]): List[Expr[TokenInfo[?]]]
\end{lstlisting}

Context bound \texttt{Type[T]} jest wykorzystywany przez \texttt{TypeRepr.of[T]} do uzyskania reprezentacji typu w API refleksji.

\subsection{Analiza złożoności}
\label{subsec:cnp-complexity}

\subsubsection{Złożoność czasowa}
\label{subsubsec:cnp-time-complexity}

Analiza złożoności algorytmu \texttt{loop}:

\begin{itemize}
    \item \textbf{Przypadek bazowy} (literał lub TermRef): \(\mathcal{O}(1)\)
    \item \textbf{Alternatywy}: \(\mathcal{O}(n)\), gdzie \(n\) to liczba alternatyw
    \item \textbf{Zagnieżdżone wiązania}: \(\mathcal{O}(d)\), gdzie \(d\) to głębokość zagnieżdżenia
\end{itemize}

Całkowita złożoność w najgorszym przypadku:

\begin{equation}
    \mathcal{O}(n \cdot d)
\end{equation}

gdzie \(n\) to liczba alternatyw, a \(d\) to głębokość zagnieżdżenia wiązań.

\subsubsection{Optymalizacja rekurencji ogonowej}
\label{subsubsec:cnp-tail-recursion}

Użycie \texttt{@tailrec} zapewnia, że rekurencja jest kompilowana do pętli, eliminując ryzyko przepełnienia stosu:

\begin{lstlisting}
@tailrec def loop(tpe: TypeRepr, pattern: Tree): List[Expr[TokenInfo[?]]] =
  (tpe, pattern) match
    case (tpe, Bind(_, tree)) => loop(tpe, tree) // (*rekurencja ogonowa*)
    case _ => (*przypadki bazowe*)
\end{lstlisting}

Kompilator weryfikuje, że wszystkie wywołania rekurencyjne są w pozycji ogonowej i zgłasza błąd, jeśli warunek nie jest spełniony.

\subsection{Wnioski}
\label{subsec:cnp-conclusions}

Klasa \texttt{CompileNameAndPattern} ilustruje kilka kluczowych aspektów zaawansowanego metaprogramowania w Scali 3:

\begin{enumerate}
    \item \textbf{Dokładność typowania:} Wykorzystanie typów singleton i typów stałych do precyzyjnego śledzenia wartości na poziomie typów

    \item \textbf{Elastyczność wzorców:} Obsługa różnorodnych form składniowych z zachowaniem spójnej semantyki

    \item \textbf{Wydajność:} Optymalizacja rekurencji ogonowej zapewniająca stałe zużycie stosu

    \item \textbf{Bezpieczeństwo:} Walidacja w czasie kompilacji zapobiegająca generacji niepoprawnego kodu

    \item \textbf{Kompozycyjność:} Czysta separacja odpowiedzialności między analizą wzorców, walidacją i konstrukcją wyników
\end{enumerate}

System demonstruje, że nawet stosunkowo prosty DSL może wymagać zaawansowanej analizy AST, ale dzięki przemyślanemu projektowi i wykorzystaniu możliwości systemu typów Scali 3, implementacja pozostaje zwięzła i łatwa w utrzymaniu.


Proces ten łączy sprawdzanie poprawności wzorca w czasie kompilacji z generacją metadanych tokena.
Wykorzystanie typu zależnego \texttt{TokenInfo[name]} zapewnia, że nazwa tokena jest śledzona na poziomie systemu typów.

\subsection{Generacja klasy anonimowej}\label{subsec:generacja-klasy-anonimowej}

\subsubsection{Definicja symboli}\label{subsubsec:definicja-symboli}

Generacja klasy implementującej \texttt{Tokenization[Ctx]} rozpoczyna się od definicji symboli:

\lstinputlisting[language=scala,caption={Definicje symboli pól tokenów},label={lst:lexer-10-decls}, linerange={134-143}]{../../src/alpaca/lexer/Lexer.scala}

Metoda \texttt{Symbol.newVal} tworzy symbol reprezentujący pole w klasie.
Parametr \texttt{parent = cls} wskazuje, że symbol jest członkiem generowanej klasy.
Flaga \texttt{Flags.Synthetic} oznacza, że symbol został wygenerowany przez kompilator, a nie pochodzi bezpośrednio z kodu źródłowego.

\subsubsection{Symbol klasy}\label{subsubsec:symbol-klasy}

Symbol klasy jest tworzony za pomocą \texttt{Symbol.newClass}:

\lstinputlisting[language=scala,caption={Tworzenie symbolu klasy anonimowej},label={lst:lexer-11-newclass}, linerange={192-198}]{../../src/alpaca/lexer/Lexer.scala}

Metoda \texttt{Symbol.newClass} przyjmuje właściciela (\texttt{Symbol.spliceOwner} — punkt ekspansji makra), nazwę (generowaną jako unikalna nazwa anonimowa), listę typów rodziców, funkcję deklarującą członków oraz opcjonalny typ \texttt{self}.

\subsubsection{Definicja ciała klasy}\label{subsubsec:definicja-ciaa-klasy}

Ciało klasy składa się z definicji wartości oraz metod:

\lstinputlisting[language=scala,caption={Ciało klasy: inicjalizacja pól tokenów i dodatkowe człony},label={lst:lexer-12-body}, linerange={200-241}]{../../src/alpaca/lexer/Lexer.scala}

\texttt{ValDef} konstruuje definicję wartości w AST. Pierwszy parametr jest symbolem pola (otrzymanym przez \texttt{cls.fieldMember}), drugi opcjonalnym wyrażeniem inicjalizującym.
Wywołanie \texttt{changeOwner} jest kluczowe dla zapewnienia, że wszystkie symbole w inicjalizatorze będą poprawnie przypisane do nowej klasy.

\subsubsection{Konstrukcja ClassDef}\label{subsubsec:konstrukcja-classdef}

Finalna konstrukcja \texttt{ClassDef} łączy wszystkie elementy:

Poniżej opisano spójną konstrukcję węzła klasy wraz z rodzicami i ciałem.
\texttt{ClassDef} przyjmuje symbol klasy, listę rodziców jako drzew typów oraz ciało jako listę definicji.
To drzewo AST reprezentuje pełną definicję klasy gotową do wstawienia w punkt ekspansji makra.

\subsection{Typy refinement i NamedTuple}\label{subsec:typy-refinement-i-namedtuple}

\subsubsection{Konstrukcja typu Fields}\label{subsubsec:konstrukcja-typu-fields}

System wykorzystuje nazwane krotki (NamedTuple) do reprezentacji dostępnych tokenów:

\lstinputlisting[language=scala,caption={Konstrukcja typu NamedTuple dla pól},label={lst:lexer-14-field-type}, linerange={145-155}]{../../src/alpaca/lexer/Lexer.scala}

Ten fragment wykorzystuje dopasowanie wzorców typów do konstruowania typu krotki na poziomie typów.
Operator \texttt{*:} reprezentuje konstruktor typu dla krotek heterogenicznych. \texttt{NamedTuple[names, types]} łączy listę nazw z listą typów, tworząc typ z nazwanymi polami.

\subsubsection{Typy refinement}\label{subsubsec:typy-refinement}

Zwracany typ jest stopniowo rafinowany dla każdego tokena:

\lstinputlisting[language=scala,caption={Rafinowanie typu wynikowego o pola tokenów},label={lst:lexer-15-refinements}, linerange={253-258}]{../../src/alpaca/lexer/Lexer.scala}

\texttt{Refinement(tpe, name, memberType)} tworzy typ refinement dodający członka o podanej nazwie i typie do bazowego typu.
Pozwala to kompilatorowi śledzić, że zwrócony obiekt ma pola odpowiadające poszczególnym tokenom, umożliwiając dostęp do nich z pełnym wsparciem systemu typów.

\subsection{Walidacja i obsługa błędów}\label{subsec:walidacja-i-obsuga-bedow}

\subsubsection{Walidacja wzorców regularnych}\label{subsubsec:walidacja-wzorcow-regularnych}

System wykorzystuje pomocniczą klasę \texttt{RegexChecker} do walidacji wzorców:

Poniższy mechanizm sprawdza poprawność składni wyrażeń regularnych już w czasie kompilacji i raportuje błędy z dokładną lokalizacją wzorca.
Metoda \texttt{report.errorAndAbort} jest częścią API kompilatora do raportowania błędów w czasie kompilacji.
Przerwanie kompilacji w przypadku niepoprawnych wzorców zapewnia, że błędy konfiguracji są wykrywane możliwie wcześnie.

\subsubsection{Obsługa nieobsługiwanych konstrukcji}\label{subsubsec:obsuga-nieobsugiwanych-konstrukcji}

Kod jawnie sygnalizuje nieobsługiwane przypadki:
Obsługiwane są wyłącznie jasno zdefiniowane formy wzorców; w przypadku napotkania innej konstrukcji kompilacja jest przerywana z komunikatem zawierającym szczegóły AST, co upraszcza diagnostykę i utrzymuje zasadę fail-fast.
Ta strategia jest zgodna z zasadą fail-fast - lepiej jest wyraźnie odrzucić nieobsługiwane konstrukcje niż milcząco generować niepoprawny kod.

\subsection{Konstrukcja finalnego wyrażenia}\label{subsec:konstrukcja-finalnego-wyrazenia}

\subsubsection{Instancetype klasy}\label{subsubsec:instancetype-klasy}

Finalna konstrukcja łączy definicję klasy z jej instancją:

Instancja klasy jest tworzona w tej samej konstrukcji \texttt{Block}, a jawne \texttt{Typed} nadaje wynikowi precyzyjny typ z rafinacjami, dzięki czemu pola tokenów są dostępne typowo bezpośrednio po ekspansji makra.

\texttt{Block(definitions, result)} tworzy blok wyrażeń w AST, gdzie definicje są wykonywane przed wyrażeniem wyniku. \texttt{Typed(term, typeTree)} jawnie adnotuje typ wyrażenia, co jest konieczne dla zapewnienia, że kompilator rozpozna typ refinement.

\subsubsection{Konwersja na Expr}\label{subsubsec:konwersja-na-expr}

Metoda \texttt{asExprOf[T]} konwertuje \texttt{Term} na \texttt{Expr[T]}, weryfikując zgodność typów.
Typ wyniku \texttt{Tokenization[Ctx] \& refinedTpe} jest typem przecięciowym (intersubsection type) łączącym bazowy interfejs z rafinacjami dla poszczególnych tokenów.

\subsection{Implikacje dla projektowania systemów metaprogramowania}\label{subsec:implikacje-dla-projektowania-systemow-metaprogramowania}

\subsubsection{Separacja poziomów abstrakcji}\label{subsubsec:separacja-poziomow-abstrakcji}

Przedstawiona implementacja demonstruje zasadę hierarchii abstrakcji opisaną przez Stuckiego.
Użytkownik końcowy operuje na prostym DSL opartym na dopasowaniu wzorców, podczas gdy implementacja wykorzystuje pełne możliwości refleksji TASTy.

\subsubsection{Bezpieczeństwo typów}\label{subsubsec:bezpieczenstwo-typow}

System zachowuje pełne bezpieczeństwo typów na wszystkich poziomach:
\begin{itemize}
    \item Wzorce są walidowane w czasie kompilacji
    \item Typy tokenów są śledzone przez typy refinement
    \item Wszystkie transformacje AST są typowane
\end{itemize}

Wykorzystanie typów zależnych i refinement pozwala na wyrażenie niezmienników systemu w samym systemie typów, eliminując całe klasy potencjalnych błędów.

\subsubsection{Wydajność}\label{subsubsec:wydajnosc}

Generowanie wyspecjalizowanego kodu w czasie kompilacji eliminuje nadmiarowe sprawdzenia i alokacje w czasie wykonania.
Wszystkie wzorce są kompilowane do wydajnych maszyn stanów, a dostęp do tokenów jest bezpośredni bez potrzeby refleksji w runtime.

\subsection{Wnioski}\label{subsec:wnioski}

Przedstawiona implementacja analizatora leksykalnego stanowi przykład zaawansowanego zastosowania systemu metaprogramowania Scali 3.
Demonstruje ona:

\begin{enumerate}
    \item \textbf{Praktyczne wykorzystanie refleksji TASTy} do analizy i transformacji AST
    \item \textbf{Generację klas w czasie kompilacji} z wykorzystaniem API \texttt{Symbol} i \texttt{ClassDef}
    \item \textbf{Dopasowanie wzorców w cytatach} do ekstrakcji informacji strukturalnej
    \item \textbf{Typy refinement i NamedTuple} do wyrażania precyzyjnych gwarancji typów
    \item \textbf{Transformację drzew AST} z zachowaniem poprawności semantycznej
\end{enumerate}

System ilustruje, jak mechanizmy metaprogramowania Scali 3 mogą być wykorzystane do budowy praktycznych narzędzi oferujących jednocześnie wygodę użytkowania i wysoką wydajność.
Przedstawione techniki są uniwersalne i mogą być zastosowane w szerokim spektrum problemów wymagających generacji kodu w czasie kompilacji.

Kluczową lekcją jest to, że choć niskopoziomowe API refleksji wymaga znacznej wiedzy technicznej, umożliwia ono budowę abstrakcji wysokiego poziomu, które są dostępne dla użytkowników końcowych bez konieczności znajomości szczegółów implementacyjnych.
To potwierdzenie zasady skalowalności systemu metaprogramowania Scali 3 - od prostych mechanizmów inline po pełną refleksję TASTy.

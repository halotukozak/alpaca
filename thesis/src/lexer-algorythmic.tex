\chapter{Algorytmy analizy leksykalnej}
\label{ch:lexer-algorythmic}

\section{Teoretyczne podstawy analizy leksykalnej}
\label{sec:lexer-theory}

Analizator leksykalny (\textit{lexer}) realizuje pierwszy etap przetwarzania tekstu źródłowego, przekształcając ciąg znaków w~strumień tokenów, które stanowią jednostki elementarne składni dla analizatora składniowego~\cite{aho2007compilers}.
Konstrukcja klasyczna wykorzystuje teorię języków formalnych oraz teorię automatów skończonych.

\subsection{Opis języka tokenów}
\label{subsec:lexer-theory-lang}
Każda klasa tokenów jest opisana przez język regularny: zbiór słów akceptowanych przez wyrażenie regularne.
Zbiór reguł tokenów tworzy sumę języków regularnych; ich unia jest również językiem regularnym, co umożliwia konstrukcję pojedynczego automatu deterministycznego.

\subsection{Automaty skończone}
\label{subsec:lexer-theory-dfa}
Wyrażenia regularne są mechanicznie tłumaczone na NFA (np.\ konstrukcją Thompsona), następnie deterministyczne DFA powstaje w~wyniku potęgowania i~ewentualnej minimalizacji stanów.
DFA konsumuje wejście znak po znaku, zachowując jednoznaczny stan aktywny i~wskazując, czy aktualny prefiks odpowiada któremukolwiek tokenowi.

\subsection{Strategia wyboru dopasowania}
\label{subsec:lexer-theory-matching}
Analizator leksykalny stosuje dwie klasyczne zasady~\cite{aho2007compilers}:
\begin{itemize}
    \item \textbf{Najdłuższe dopasowanie} (\textit{maximal munch}) --- dopóki DFA ma ścieżkę przejść, znak jest konsumowany; token jest generowany w~oparciu o~ostatni osiągnięty stan akceptujący na danej ścieżce.
    \item \textbf{Priorytet reguł} --- gdy kilka reguł akceptuje prefiks o~tej samej długości, wybierana jest reguła o~najwyższym priorytecie (często określanym kolejnością definicji).
\end{itemize}
Te zasady zapewniają deterministyczność strumienia tokenów przy zachowaniu intuicyjnej semantyki wzorców.

\subsection{Błędy leksykalne}
\label{subsec:lexer-theory-errors}
Jeśli DFA nie ma przejścia dla bieżącego znaku, zgłaszany jest błąd leksykalny w~bieżącej pozycji.

\section{Automaty DFA a wyrażenia regularne w~Alpaca}
\label{sec:dfa-vs-regex}

Kanoniczna metoda budowy analizatora leksykalnego zakłada konstrukcję deterministycznego automatu skończonego z~zestawu wyrażeń regularnych, jednak w~\texttt{alpaca.lexer} podejście to zostało zastąpione praktycznym rozwiązaniem opartym na bibliotecznym silniku wyrażeń regularnych. Zamiast ręcznie kodować DFA, makro kompilacyjne łączy wszystkie wzorce operatorem alternatywy \texttt{|} w~jedno wyrażenie regularne z~nazwanymi grupami, co umożliwia identyfikację dopasowanej reguły poprzez pojedyncze wywołanie \texttt{findPrefixMatchOf}.

\subsection{Zalety podejścia}
\label{subsec:pattern-matching-pros}
\begin{itemize}
    \item \textbf{Niższy próg wejścia} --- dzięki wykorzystaniu dobrze znanego mechanizmu wyrażeń regularnych, użytkownicy DSL mogą definiować reguły leksykalne bez konieczności zrozumienia złożoności konstrukcji i~optymalizacji DFA.
    \item \textbf{Elastyczność wzorców} --- użytkownicy mogą korzystać z~rozszerzeń wyrażeń regularnych (np.\ \textit{backreference}, \textit{negative lookahead}, czy \textit{conditional}), których implementacja nie jest możliwa za pomocą DFA.
    \item \textbf{Stabilność} --- własna implementacja DFA pozwala na implementację jedynie podzbioru funkcjonalności oraz wymaga ciągłego utrzymania i~aktualizacji w~miarę ewolucji języka i~jego wymagań. Alpaca korzysta ze zoptymalizowanego i~sprawdzonego silnika wyrażeń regularnych.
    \item \textbf{Spójność z~platformą} --- wykorzystanie natywnego silnika wyrażeń regularnych JVM/Scali zapewnia lepszą integrację z~innymi narzędziami i~bibliotekami ekosystemu, co ułatwia debugowanie i~rozwój.
    \item \textbf{Lepsza czytelność kodu} --- definicje reguł leksykalnych pozostają zwięzłe i~zrozumiałe, co ułatwia ich przeglądanie i~modyfikację.
\end{itemize}

\subsection{Wady i ograniczenia}
\label{subsec:pattern-matching-cons}
\begin{itemize}
    \item \textbf{Wydajność} --- silniki wyrażeń regularnych mogą być mniej wydajne niż ręcznie zoptymalizowane DFA, zwłaszcza w~przypadku bardzo dużych zestawów reguł.
    \item \textbf{Brak pełnej kontroli nad strategią dopasowania} --- przy silniku wyrażeń regularnych trzeba dostosować się do jego wyboru prefiksu, co może prowadzić do nieoczekiwanych zachowań. Przykładem jest sytuacja, gdy do wzorca dopasują się dwie reguły różnej długości, a~silnik wybierze krótszą z~nich.
\end{itemize}

Podsumowując, wybór natywnego dopasowania wzorców upraszcza interfejs użytkownika, zwiększa elastyczność w~kształtowaniu składni oraz ułatwia tworzenie i~utrzymanie bez znajomości szczegółów implementacyjnych automatu, za cenę szybkości wykonania.

\section{Praktyczna implementacja analizatora leksykalnego w~Alpaca}
\label{sec:lexer-impl}

Implementacja \texttt{alpaca.lexer} łączy wygodę DSL z~kodem wykonywanym w~czasie kompilacji i~lekkim przebiegiem wykonania.
Makro \texttt{lexer} kompiluje reguły do jednego wyrażenia regularnego z~nazwanymi grupami, a~następnie generuje klasę \texttt{Tokenization}, która realizuje właściwe skanowanie.


\subsection{Przebieg tokenizacji w~Alpaca}
\label{subsec:lexer-impl-flow}
Podczas kompilacji wszystkie wzorce są łączone operatorem alternatywy \texttt{|} w~jeden wzorzec z~nazwanymi grupami, co pozwala odróżnić, która reguła dopasowała się podczas pojedynczego przebiegu \texttt{findPrefixMatchOf}.

Następnie pętla skanująca wywołuje ten wzorzec na wejściu, wybiera dopasowany token, akumuluje leksemy i~przesuwa wskaźnik wejścia, aż do wyczerpania danych.

\subsection{Obsługa ignorowanych reguł}
\label{subsec:lexer-impl-ignored}
Alpaca pozwala oznaczać reguły jako ,,ignorowane''. Takie reguły są wbudowywane we wspólny wzorzec, ale ich dopasowania nie tworzą leksemów.
Ujednolicony przebieg pętli upraszcza kod wykonawczy --- ignorowane tokeny różnią się tylko akcją po dopasowaniu, czyli brakiem generacji leksemu.

\subsection{Kontekst i rozszerzenia}
\label{subsec:lexer-impl-context}
Możliwa jest stanowa analiza leksykalna poprzez trzymany w~kontekście stan maszyny.
Alpaca wewnętrznie definiuje mechanizm \texttt{BetweenStages}, który jest wywoływany po każdym dopasowaniu leksemu i~pozwala modyfikować kontekst.
Domyślna implementacja zapamiętuje ostatni leksem oraz śledzi numer linii i~kolumny, jednak użytkownik może rozszerzyć tę logikę o~własne zachowania, np.\ weryfikację poprawności użycia nawiasów w~kodzie.

\subsection{Błędy leksykalne}
\label{subsec:lexer-impl-errors}
Gdy wyrażenie regularne nie znajduje prefiksu, analizator leksykalny zgłasza błąd z~informacją o~pierwszym nieoczekiwanym znaku. Dzięki kontekstowi uzupełnionemu przez \texttt{BetweenStages} można raportować linię, kolumnę i~ostatni prawidłowy leksem, co znacząco poprawia diagnostykę podczas analizy leksykalnej.

\subsection{Strumieniowe czytanie wejścia}
\label{subsec:lexer-impl-lazy-reader}
W~celu optymalizacji ilości operacji wejścia/wyjścia, analizator leksykalny może analizować wejście w~sposób strumieniowy. Zapewnia to wykorzystanie interfejsu \texttt{CharSequence} implementowanego przez klasę \texttt{LazyReader}, która pobiera dane z~pliku w~16KB blokach (\textit{chunks}) i~buforuje je lokalnie. Metoda \texttt{ensure} dba o~to, aby żądana pozycja była dostępna w~buforze, czytając kolejne porcje danych w~razie potrzeby.

\lstinputlisting[language=scala,caption={Implementacja metody ensure w~klasie LazyReader},label={lst:lazy-reader-ensure-impl}]{listings/lexer-algorythmic/lazy-reader-ensure.scala}

\subsection{Weryfikacja wzorców}
\label{subsec:lexer-impl-regexchecker}
Przed wygenerowaniem analizatora leksykalnego, makro kompilacyjne uruchamia moduł \texttt{RegexChecker}, który analizuje wzorce pod kątem pokrywania reguł. Wykrywa on dwa typy problemów:
\begin{itemize}
    \item \textbf{Subsumpcja}~\cite{brzozowski1964derivatives} --- sytuacja, w~której późniejszy wzorzec jest całkowicie pokryty przez wcześniejszy.
    \item \textbf{Pokrycie prefiksów} --- sytuacja, w~której wzorce nakładają się na siebie częściowo, co może prowadzić do niejednoznaczności w~dopasowaniu wzorca przez silnik wyrażeń regularnych.
\end{itemize}
Wykrycie jednej z~powyższych zależności powoduje błąd kompilacji z~czytelnym i~jasnym komunikatem, dzięki czemu nieosiągalne tokeny są eliminowane przed uruchomieniem programu.

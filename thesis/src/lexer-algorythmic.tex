\chapter{Algorytmika Leksera}
\label{ch:lexer-algorythmic}

\section{Teoretyczne podstawy działania leksera}
\label{sec:lexer-theory}

Lekser (analizator leksykalny) realizuje pierwszy etap przetwarzania źródła: przekształca ciąg znaków w strumień tokenów, które są „atomami” składni dla parsera.
Klasyczna konstrukcja opiera się na połączeniu teorii formalnych języków i automatów skończonych.

\subsection{Opis języka tokenów}
\label{subsec:lexer-theory-lang}
Każda klasa tokenów jest opisana przez język regularny: zbiór słów akceptowanych przez wyrażenie regularne.
Zbiór reguł tokenów tworzy sumę języków regularnych; ich unia jest również językiem regularnym, co pozwala kompilować je do jednego automatu deterministycznego.

\subsection{Automaty skończone}
\label{subsec:lexer-theory-dfa}
Wyrażenia regularne są mechanicznie tłumaczone na NFA (np. konstrukcją Thompsona), następnie deterministyczne DFA powstaje w wyniku potęgowania i ewentualnej minimalizacji stanów.
DFA konsumuje wejście znak po znaku, zachowując jednoznaczny stan aktywny i wskazując, czy aktualny prefiks odpowiada któremukolwiek tokenowi.

\subsection{Strategia wyboru dopasowania}
\label{subsec:lexer-theory-matching}
Lekser stosuje dwie klasyczne zasady:
\begin{itemize}
    \item \textbf{Najdłuższe dopasowanie (maximal munch)} — dopóki DFA ma ścieżkę przejść, znak jest konsumowany; token jest emitowany dopiero po ostatnim stanie akceptującym widzianym na tej ścieżce.
    \item \textbf{Priorytet reguł} — gdy kilka reguł akceptuje prefiks o tej samej długości, wybierana jest reguła o najwyższym priorytecie (często określanym kolejnością definicji).
\end{itemize}
Te zasady zapewniają deterministyczność strumienia tokenów przy zachowaniu intuicyjnej semantyki wzorców.

\subsection{Błędy leksykalne}
\label{subsec:lexer-theory-errors}
Jeśli DFA nie ma przejścia dla bieżącego znaku, ogłasza błąd leksykalny w bieżącej pozycji.

\section{Automaty DFA a wyrażenia regularne w Alpaca}
\label{sec:dfa-vs-regex}

Kanoniczna metoda budowy leksera zakłada konstrukcję deterministycznego automatu skończonego z zestawu wyrażeń regularnych, jednak w \texttt{alpaca.lexer} podejście to zostało zastąpione praktycznym rozwiązaniem opartym na bibliotecznym silniku regexów. Zamiast ręcznie kodować DFA, makro kompilacyjne łączy wszystkie wzorce operatorem alternatywy \texttt{|} w jedno wyrażenie regularne z nazwanymi grupami, co umożliwia identyfikację dopasowanej reguły poprzez pojedyncze wywołanie \texttt{findPrefixMatchOf}.

\subsection{Zalety podejścia}
\label{subsec:pattern-matching-pros}
\begin{itemize}
    \item \textbf{Niższy próg wejścia} — dzięki wykorzystaniu dobrze znanego mechanizmu regex, użytkownicy DSL mogą definiować reguły leksykalne bez konieczności zrozumienia złożoności konstrukcji i optymalizacji DFA.
    \item \textbf{Elastyczność wzorców} — użytkownicy mogą korzystać z rozszerzeń regex (np. backreference, negative lookahead, czy conditional ) których implementacja nie jest możliwa za pomocą DFA.
    \item \textbf{Stabilność} - Własna implementacja DFA pozwala na implementację jedynie podzbioru funkcjonalności, oraz wymaga ciągłego utrzymania i aktualizacji w miarę ewolucji języka i jego wymagań. Alpaca korzysta ze zoptymalizowanego i sprawdzonego silnika regex.
    \item \textbf{Spójność z platformą} - Wykorzystanie natywnego silnika regex JVM/Scali zapewnia lepszą integrację z innymi narzędziami i bibliotekami ekosystemu, co ułatwia debugowanie i rozwój.
    \item \textbf{Lepsza czytelność kodu} — definicje reguł leksykalnych pozostają zwięzłe i zrozumiałe, co ułatwia ich przeglądanie i modyfikację.
\end{itemize}

\subsection{Wady i ograniczenia}
\label{subsec:pattern-matching-cons}
\begin{itemize}
    \item \textbf{Wydajność} — silniki regex mogą być mniej wydajne niż ręcznie zoptymalizowane DFA, zwłaszcza w przypadku bardzo dużych zestawów reguł.
    \item \textbf{Brak pełnej kontroli nad strategią dopasowania} — przy silniku regex trzeba dostosować się do jego wyboru prefiksu, co może prowadzić do nieoczekiwanych zachowań. Przykładem jest sytuacja, gdy do wzorca dopasują się dwie reguły różnej długości, a silnik wybierze krótszą z nich.
\end{itemize}

Podsumowując, wybór natywnego dopasowania wzorców upraszcza interfejs użytkownika, zwiększa elastyczność w kształtowaniu składni, oraz ułatwia tworzenie i utrzymanie bez znajomości szczegółów implementacyjnych automatu za cenę szybkości wykonania.

\section{Praktyczna implementacja leksera w Alpaca}
\label{sec:lexer-impl}

Implementacja \texttt{alpaca.lexer} łączy wygodę DSL z kodem wykonywanym w czasie kompilacji i lekki przebieg wykonania.
Makro \texttt{lexer} kompiluje reguły do jednego wyrażenia regularnego z nazwanymi grupami, a następnie generuje klasę \texttt{Tokenization}, która realizuje właściwe skanowanie.


\subsection{Przebieg tokenizacji w Alpaca}
\label{subsec:lexer-impl-flow}
Podczas kompisalcji wszystkie wzorce są łączone operatorm alternatywy \texttt{|} w jeden regex z nazwanymi grupami, co pozwala odróżnić, która reguła dopasowała się podczas pojedynczego przebiegu \texttt{findPrefixMatchOf}.

Następnie pętla skanująca wywołuje ten regex na wejściu, wybiera dopasowany token, akumuluje lexemy i przesuwa wskaźnik wejścia, aż do wyczerpania danych.

\subsection{Obsługa ignorowanych reguł}
\label{subsec:lexer-impl-ignored}
Alpaca pozwala oznaczać reguły jako „ignorowane”. Takie reguły są wbudowywane we wspólny wzorzec, ale ich dopasowania nie tworzą lexemów.
Ujednolicony przebieg pętli upraszcza kod wykonawczy: ignorowane tokeny różnią się tylko akcją po dopasowaniu (brak emisji lexemu).

\subsection{Kontekst i rozszerzenia}
\label{subsec:lexer-impl-context}
Możliwa jest stanowa analiza leksykalna poprzez trzymany w kontekście stan maszyny.
Alpaca wewnętrznie definiuje mechanizm \texttt{BetweenStages}, który jest wywoływany po każdym dopasowaniu leksema i pozwala modyfikować kontekst.
Domyślna implementacja zapamiętuje ostatni leksem, oraz śledzi numer linii i kolumny, jednak użytkownik może rozszerzyć tę logikę o własne zachowania (np.: weryfikację poprawności użycia nawiasów w kodzie).

\subsection{Błędy leksykalne}
\label{subsec:lexer-impl-errors}
Gdy regex nie znajduje prefiksu, lekser zgłasza błąd z informacją o pierwszym nieoczekiwanym znaku. Dzięki kontekstowi uzupełnionemu przez \texttt{BetweenStages} można raportować linię, kolumnę i ostatni prawidłowy lexem, co znacząco poprawia diagnostykę.

\subsection{Strumieniowe czytanie wejścia}
\label{subsec:lexer-impl-lazy-reader}
W celu uniknięcia nadmiernego zużycia pamięci, lekser analizuje wejście w sposób strumieniowy. Zapewnia to wykorzystanie interfejsu \texttt{CharSequence} implementowanego przez klasę \texttt{LazyReader}. W celu optymalizacji ilości operacji wejścia wyjścia pobiera ona dane z pliku w 16KB blokach (chunkach) i buforuje je lokalnie. Metoda \texttt{ensure} dba o to, aby żądana pozycja była dostępna w buforze, czytając kolejne porcje danych w razie potrzeby.

\begin{lstlisting}[language=scala,caption={Implementacja metody ensure w klasie LazyReader},label={lst:lazy-reader-ensure-impl}]
  @tailrec
  private def ensure(pos: Int): Unit =
    if pos >= buffer.length then
      val charsRead = reader.read(chunk)
      if charsRead == -1 then
      throw new IndexOutOfBoundsException(s"Position $pos is out of bounds for LazyReader of size $size")
    else
      buffer.appendAll(chunk.iterator.take(charsRead))
      ensure(pos)
\end{lstlisting}

\subsection{Weryfikacja wzorców}
\label{subsec:lexer-impl-regexchecker}
Przed wygenerowaniem skanera, makro kompilacyjne uruchamia moduł \texttt{RegexChecker}, który analizuje wzorce pod kątem pokrywania reguł. Wykrywa ono dwa typy problemów:
\begin{itemize}
    \item \textbf{Subsumpcję} — sytuację, w której późniejszy wzorzec jest całkowicie pokryty przez wcześniejszy.
    \item \textbf{Pokrycie prefiksów} — sytuację, w której wzorce nakładają się na siebie częściowo, co może prowadzić do niejednoznaczności w dopasowaniu wzorca przez silnik wyrażeń regularnych.
\end{itemize}
Wykrycie jednej z powyższych zależności powoduje błąd kompilacji z czytelnym i jasnym komunikatem, dzięki czemu „martwe” tokeny są eliminowane przed uruchomieniem programu.

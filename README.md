# Alpaca ü¶ô ‚Äî A Lexer Parser And Compiler

A modern, type-safe lexer and parser library for Scala 3, featuring compile-time validation and elegant DSL syntax.

## Features

- üîç **Type-safe lexer and parser** ‚Äî Catch errors at compile time with Scala 3's powerful type system
- üéØ **Elegant DSL** ‚Äî Define lexers and parsers using intuitive pattern matching syntax
- ‚ö° **Compile-time validation** ‚Äî Regex patterns and grammar rules are validated during compilation
- üß™ **Macro-based** ‚Äî Leverages Scala 3 macros for efficient code generation
- üìö **Context-aware** ‚Äî Support for lexical and parsing contexts with type-safe state management
- üõ†Ô∏è **LR Parsing** ‚Äî Uses LR parsing algorithm with automatic parse table generation

## Quick Navigation

### For Getting Started
- üìñ [Getting Started Guide](./docs/_docs/getting-started.md) ‚Äî Installation and basic usage
- üöÄ [Lexer Quickstart](./docs/_docs/lexer-quickstart.md) ‚Äî Copy-paste examples for common patterns

### For Lexer Development
- üìö [Lexer Development Guide](./docs/_docs/lexer-development.md) ‚Äî Comprehensive reference covering API, context, and testing
- üîß [Lexer Internals](./docs/_docs/lexer-internals.md) ‚Äî Macro implementation, type system, and advanced customization
- üìã [Lexer API Reference](./docs/_docs/lexer-api-reference.md) ‚Äî Complete type signatures and API documentation

### For Parser Development
- üìò Parser Development Guide (Coming Soon)

## Installation

### Mill

Add Alpaca as a dependency in your `build.mill`:

```mill
//| mill-version: 1.0.6
//| mill-jvm-version: 21

import mill._
import mill.scalalib._

object myproject extends ScalaModule {
  def scalaVersion = "3.7.4"
  
  def mvnDeps = Seq(
    mvn"io.github.halotukozak::alpaca:0.0.1"
  )
}
```

### SBT

Add Alpaca to your `build.sbt`:

```sbt
libraryDependencies += "io.github.halotukozak" %% "alpaca" % "0.0.1"
scalaVersion := "3.7.4"
```

### Scala CLI

Use Alpaca directly in your Scala CLI scripts:

```scala
//> using scala "3.7.4"
//> using dep "io.github.halotukozak::alpaca:0.0.1"

import alpaca.*

// Your code here
```

## 30-Second Example

### Defining a Lexer

```scala
import alpaca.*

val Lexer = lexer {
  case num @ "[0-9]+" => Token["int"](num.toInt)
  case "\\+" => Token["plus"]
  case "-" => Token["minus"]
  case "\\s+" => Token.Ignored
}
```

### Using the Lexer

```scala
val (ctx, lexemes) = Lexer.tokenize("1 + 2 - 3")

lexemes.foreach { lexeme =>
  println(s"${lexeme.name}: ${lexeme.value}")
}

// Output:
// int: 1
// plus: ()
// int: 2
// minus: ()
// int: 3
```

See the [Lexer Quickstart](./docs/_docs/lexer-quickstart.md) for more examples.

## Project Structure

```text
alpaca/
‚îú‚îÄ‚îÄ src/alpaca/
‚îÇ   ‚îú‚îÄ‚îÄ internal/              # Internal implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lexer/            # Lexer internals (Token, Lexem, Tokenization, etc.)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parser/           # Parser internals (ParseTable, State, Item, etc.)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Empty.scala       # Empty type class utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Copyable.scala    # Copyable type class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Showable.scala    # Showable type class for debugging
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...               # Other core utilities
‚îÇ   ‚îú‚îÄ‚îÄ lexer.scala           # Public lexer DSL and API
‚îÇ   ‚îú‚îÄ‚îÄ parser.scala          # Public parser DSL and API
‚îÇ   ‚îî‚îÄ‚îÄ local.scala           # Local utilities
‚îú‚îÄ‚îÄ test/src/alpaca/          # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ LexerApiTest.scala    # Lexer tests
‚îÇ   ‚îú‚îÄ‚îÄ ParserApiTest.scala   # Parser tests
‚îÇ   ‚îî‚îÄ‚îÄ integration/          # Integration tests
‚îú‚îÄ‚îÄ example/                  # Example projects
‚îú‚îÄ‚îÄ docs/                     # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ _docs/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ getting-started.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lexer-development.md          # Comprehensive lexer guide
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lexer-quickstart.md           # Practical examples
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lexer-internals.md            # Macro and implementation details
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lexer-api-reference.md        # Complete API reference
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ _assets/              # Documentation assets
‚îú‚îÄ‚îÄ build.mill                # Mill build configuration
‚îî‚îÄ‚îÄ README.md                 # This file
```

## Documentation

### Lexer Documentation

The Alpaca lexer system is thoroughly documented:

| Document | Audience | Level |
|----------|----------|-------|
| [Lexer Quickstart](./docs/_docs/lexer-quickstart.md) | All users | Beginner |
| [Lexer Development Guide](./docs/_docs/lexer-development.md) | DSL users, language designers | Intermediate |
| [Lexer Internals](./docs/_docs/lexer-internals.md) | Contributors, advanced users | Advanced |
| [Lexer API Reference](./docs/_docs/lexer-api-reference.md) | API consumers | Reference |

### Typical Learning Path

1. **Start with** [Getting Started](./docs/_docs/getting-started.md) for installation
2. **Explore** [Lexer Quickstart](./docs/_docs/lexer-quickstart.md) for hands-on examples
3. **Deep dive** [Lexer Development Guide](./docs/_docs/lexer-development.md) for comprehensive knowledge
4. **Reference** [Lexer API Reference](./docs/_docs/lexer-api-reference.md) while coding
5. **Understand internals** [Lexer Internals](./docs/_docs/lexer-internals.md) for extending or contributing

## Key Concepts

### Tokens

Tokens are the building blocks of a lexer. Define them using regex patterns and extract values:

```scala
val Lexer = lexer {
  case num @ "[0-9]+" => Token["int"](num.toInt)      // Extract Int
  case id @ "[a-z]+" => Token["id"](id)                // Extract String
  case "#.*" => Token.Ignored                           // Skip comments
}
```

### Context

Lexical context tracks state during tokenization:

```scala
case class MyCtx(
  var text: CharSequence = "",
  var line: Int = 1,
  var parenDepth: Int = 0,
) extends LexerCtx

val Lexer = lexer[MyCtx] {
  case "(" => ctx.parenDepth += 1; Token["lparen"]
  case ")" => ctx.parenDepth -= 1; Token["rparen"]
  case "\n" => ctx.line += 1; Token.Ignored
}
```

### Pattern Ordering

Patterns match in declaration order. More specific patterns must come first:

```scala
val Lexer = lexer {
  case "[0-9]+\\.[0-9]+" => Token["float"]   // Specific: floats
  case "[0-9]+" => Token["int"]               // General: integers
  
  case "if" | "else" => Token["keyword"]     // Keywords before
  case "[a-z]+" => Token["id"]                // Identifiers
}
```

## Examples

See the `example/` directory for complete working projects, including:

- **Math Expression Parser**: Simple arithmetic with precedence
- **Configuration Language**: TOML-like format with nesting
- **Mini Language**: Function definitions, loops, and variables

## Building from Source

### Prerequisites

- JDK 21 or later
- Mill 1.0.6 or later

### Build Commands

```bash
# Compile the project
./mill compile

# Run tests
./mill test

# Generate documentation
./mill docJar

# Run test coverage
./mill test.scoverage.htmlReport
```

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

For more details, see the [Contributing Guidelines](./CONTRIBUTING.md) (if present).

## Authors

Created by [halotukozak](https://github.com/halotukozak) and [Corvette653](https://github.com/Corvette653)

## License

MIT License ‚Äî See [LICENSE](./LICENSE) for details

---

Made with ‚ù§Ô∏è and coffee ‚Üí

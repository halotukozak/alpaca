package alpaca
package internal
package lexer

import alpaca.internal.lexer.ErrorHandling.Strategy

import scala.NamedTuple.NamedTuple
import scala.annotation.tailrec
import scala.util.matching.Regex

/**
 * The result of compiling a lexer definition.
 *
 * This abstract class represents a compiled lexer that can tokenize input.
 * It is generated by the `lexer` macro and provides methods to access tokens
 * and perform tokenization.
 *
 * @tparam Ctx the global context type
 */
transparent abstract class Tokenization[Ctx <: LexerCtx](
  using betweenStages: BetweenStages[Ctx],
  errorHandling: ErrorHandling[Ctx],
  empty: Empty[Ctx],
) extends Selectable:
  type LexemeRefinement <: Lexeme[?, ?]

  /** List of all tokens defined in this lexer, including ignored tokens. */
  def tokens: List[Token[?, Ctx, ?]]

  /**
   * Provides dynamic access to tokens by name.
   *
   * This allows accessing tokens using dot notation, e.g., `myLexer.PLUS`.
   *
   * @param fieldName the token name
   * @return the token definition
   */
  def selectDynamic(fieldName: String): DefinedToken[?, Ctx, ?] { type LexemeTpe = LexemeRefinement }

  /**
   * Tokenizes the input character sequence.
   *
   * Processes the input from start to finish, matching tokens and building
   * a list of lexems. Throws a RuntimeException if an unexpected character
   * is encountered.
   *
   * @param input the input to tokenize
   * @param empty implicit Empty instance to create the initial context
   * @return a list of lexems representing the tokenized input
   */
  final def tokenize(input: CharSequence): (ctx: Ctx, lexemes: List[Lexeme[?, ?] & LexemeRefinement]) =
    @tailrec def loop(
      globalCtx: Ctx,
    )(
      acc: List[Lexeme[?, ?] & LexemeRefinement],
    ): List[Lexeme[?, ?] & LexemeRefinement] =
      globalCtx.text.length match
        case 0 =>
          acc.reverse // todo: make it not reversed
        case _ =>
          val matcher = compiled.matcher(globalCtx.text)

          val (token, matched) = if matcher.lookingAt then
            val matched = matcher.group(0)
            globalCtx.lastRawMatched = matched
            globalCtx.text = globalCtx.text.from(matcher.end)
            Iterator
              .range(1, matcher.groupCount + 1)
              .collectFirst:
                case i if matcher.start(i) != -1 => (groupToTokenMap(i), matched)
              .getOrElse:
                throw new AlgorithmError(s"${matcher.pattern} matched but no token defined for it")
          else
            errorHandling(globalCtx) match
              case Strategy.Throw(ex) =>
                throw ex

              case Strategy.IgnoreToken if matcher.hasMatch =>
                val firstMatching = matcher.start
                val matched = globalCtx.text.subSequence(0, firstMatching).toString
                globalCtx.lastRawMatched = matched
                globalCtx.text = globalCtx.text.from(firstMatching)
                (RecoveredToken(matched), matched)

              case Strategy.IgnoreChar | Strategy.IgnoreToken =>
                val matched = globalCtx.text.charAt(0).toString
                globalCtx.lastRawMatched = matched
                globalCtx.text = globalCtx.text.from(1)
                (RecoveredToken(matched), matched)

              case Strategy.Stop =>
                globalCtx.text = ""
                return loop(globalCtx)(acc)

          betweenStages(token, matched, globalCtx)
          val lexem = List(token).collect:
            case _: DefinedToken[?, Ctx, ?] => globalCtx.lastLexeme.nn.asInstanceOf[Lexeme[?, ?] & LexemeRefinement]
          loop(globalCtx)(lexem ::: acc)

    val initialContext = empty()
    initialContext.text = input
    (initialContext, loop(initialContext)(Nil))

  /** The compiled pattern that matches all defined tokens. */
  protected def compiled: java.util.regex.Pattern

  private lazy val groupToTokenMap: Array[Token[?, Ctx, ?]] =
    val matcher = compiled.matcher("")
    val totalGroups = matcher.groupCount
    val map = new Array[Token[?, Ctx, ?]](totalGroups + 1)

    tokens.iterator.foreach: token =>
      val groupIndex = compiled.namedGroups.get(token.info.regexGroupName)
      if groupIndex != null then map(groupIndex) = token
    map

extension (input: CharSequence)
  private[alpaca] def from(pos: Int): CharSequence = input match
    case lfr: LazyReader => lfr.from(pos)
    case _ => input.subSequence(pos, input.length)

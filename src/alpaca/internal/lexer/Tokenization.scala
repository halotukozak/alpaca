package alpaca
package internal
package lexer

import scala.annotation.tailrec
import scala.util.matching.Regex

/**
 * The result of compiling a lexer definition.
 *
 * This abstract class represents a compiled lexer that can tokenize input.
 * It is generated by the `lexer` macro and provides methods to access tokens
 * and perform tokenization.
 *
 * @tparam Ctx the global context type
 */
abstract class Tokenization[Ctx <: LexerCtx: {Copyable as copy, BetweenStages as betweenStages}] extends Selectable {

  /** List of all tokens defined in this lexer, including ignored tokens. */
  def tokens: List[Token[?, Ctx, ?]]

  /** Map of token names to their definitions for dynamic access. */
  def byName: Map[String, DefinedToken[?, Ctx, ?]] // todo: reconsider if selectDynamic should be implemented with PM

  /**
   * Provides dynamic access to tokens by name.
   *
   * This allows accessing tokens using dot notation, e.g., `myLexer.PLUS`.
   *
   * @param fieldName the token name
   * @return the token definition
   */
  def selectDynamic(fieldName: String): DefinedToken[?, Ctx, ?] =
    byName(scala.reflect.NameTransformer.decode(fieldName))

  /**
   * Tokenizes the input character sequence.
   *
   * Processes the input from start to finish, matching tokens and building
   * a list of lexems. If the context extends `LexerErrorRecovery`, unexpected
   * characters are collected as errors and lexing continues. Otherwise, a
   * RuntimeException is thrown for unexpected characters.
   *
   * @param input the input to tokenize
   * @param empty implicit Empty instance to create the initial context
   * @return a list of lexems representing the tokenized input
   */
  final def tokenize(input: CharSequence)(using empty: Empty[Ctx]): List[Lexem[?, ?]] = {
    @tailrec def loop(globalCtx: Ctx)(acc: List[Lexem[?, ?]]): List[Lexem[?, ?]] =
      globalCtx.text.length match
        case 0 =>
          acc.reverse
        case _ =>
          compiled.findPrefixMatchOf(globalCtx.text) match
            case Some(m) =>
              val token = tokens.find(token => m.group(token.info.regexGroupName) ne null) getOrElse {
                throw new AlgorithmError(s"$m matched but no token defined for it")
              }
              betweenStages(token, m, globalCtx)
              val lexem = List(token).collect:
                case _: DefinedToken[?, Ctx, ?] => globalCtx.lastLexem
              loop(globalCtx)(lexem ::: acc)

            case None =>
              // No match found - handle error recovery or throw
              globalCtx match
                case errCtx: LexerErrorRecovery =>
                  // Record the error with position info if available
                  val position = globalCtx match
                    case pt: PositionTracking => pt.position
                    case _                    => 0
                  val line = globalCtx match
                    case lt: LineTracking => lt.line
                    case _                => 0
                  errCtx.lexerErrors += SyntaxError.UnexpectedChar(
                    globalCtx.text.charAt(0),
                    position,
                    line,
                  )
                  // Skip the problematic character and continue
                  globalCtx.text = globalCtx.text.from(1)
                  // Update position tracking if available
                  globalCtx match
                    case pt: PositionTracking => pt.position += 1
                    case _                    => ()
                  loop(globalCtx)(acc)

                case _ =>
                  throw new RuntimeException(s"Unexpected character: '${globalCtx.text.charAt(0)}'")

    val initialContext = empty()
    initialContext.text = input
    loop(initialContext)(Nil)
  }

  /** The compiled regex that matches all defined tokens. */
  protected def compiled: Regex
}

extension (input: CharSequence)
  private[alpaca] def from(pos: Int): CharSequence = input match
    case lfr: LazyReader => lfr.from(pos)
    case _ => input.subSequence(pos, input.length)

package alpaca
package internal
package lexer

import scala.NamedTuple.{AnyNamedTuple, NamedTuple}
import scala.annotation.tailrec

/**
 * The result of compiling a lexer definition.
 *
 * This abstract class represents a compiled lexer that can tokenize input.
 * It is generated by the `lexer` macro and provides methods to access tokens
 * and perform tokenization.
 *
 * @tparam Ctx the global context type
 */
transparent abstract class Tokenization[Ctx <: LexerCtx](
  using betweenStages: BetweenStages[Ctx],
) extends Selectable:
  type Fields <: AnyNamedTuple
  type LexemeFields <: AnyNamedTuple
  final type Lexeme = alpaca.internal.lexer.Lexeme[?, ?] withFields LexemeFields

  /** List of all tokens defined in this lexer, including ignored tokens. */
  def tokens: List[Token[?, Ctx, ?]]

  /**
   * Provides dynamic access to tokens by name.
   *
   * This allows accessing tokens using dot notation, e.g., `myLexer.PLUS`.
   *
   * @param fieldName the token name
   * @return the token definition
   */
  def selectDynamic(fieldName: String): DefinedToken[?, Ctx, ?] { type LexemeTpe = Lexeme }

  /**
   * Tokenizes the input character sequence.
   *
   * Processes the input from start to finish, matching tokens and building
   * a list of lexems. Throws a RuntimeException if an unexpected character
   * is encountered.
   *
   * @param input the input to tokenize
   * @param empty implicit Empty instance to create the initial context
   * @return a list of lexems representing the tokenized input
   */
  final def tokenize(
    input: CharSequence,
  )(using empty: Empty[Ctx],
  ): (ctx: Ctx, lexemes: List[Lexeme]) =
    @tailrec def loop(globalCtx: Ctx)(acc: List[Lexeme]): List[Lexeme] =
      globalCtx.text.length match
        case 0 =>
          acc.reverse // todo: make it not reversed
        case _ =>
          val matcher = compiled.matcher(globalCtx.text)

          val token =
            if matcher.lookingAt then
              Iterator
                .range(1, matcher.groupCount + 1)
                .collectFirst:
                  case i if matcher.start(i) != -1 => groupToTokenMap(i)
                .getOrElse:
                  throw new AlgorithmError(s"${matcher.pattern} matched but no token defined for it")
            else
              // todo: custom error handling https://github.com/halotukozak/alpaca/issues/21
              throw new RuntimeException(s"Unexpected character: '${globalCtx.text.charAt(0)}'")
          betweenStages(token, matcher, globalCtx)
          val lexem = List(token).collect:
            case _: DefinedToken[?, Ctx, ?] => globalCtx.lastLexeme.nn.asInstanceOf[Lexeme]
          loop(globalCtx)(lexem ::: acc)

    val initialContext = empty()
    initialContext.text = input
    (initialContext, loop(initialContext)(Nil))

  /** The compiled pattern that matches all defined tokens. */
  protected def compiled: java.util.regex.Pattern

  private lazy val groupToTokenMap: Array[Token[?, Ctx, ?]] =
    val matcher = compiled.matcher("")
    val totalGroups = matcher.groupCount
    val map = new Array[Token[?, Ctx, ?]](totalGroups + 1)

    tokens.iterator.foreach: token =>
      val groupIndex = compiled.namedGroups.get(token.info.regexGroupName)
      if groupIndex != null then map(groupIndex) = token
    map

extension (input: CharSequence)
  private[alpaca] def from(pos: Int): CharSequence = input match
    case lfr: LazyReader => lfr.from(pos)
    case _ => input.subSequence(pos, input.length)

# The Compilation Pipeline

Source text is just a string. A compiler pipeline is a sequence of transformations that turns that string into something structured and meaningful. Each stage takes the output of the previous one, narrowing the representation from raw text to a typed result.

Understanding the pipeline gives you a mental model that applies to every Alpaca program you write — not just calculator expressions, but any language you define with the library.

## The Four Stages

Most compilers share the same four-stage structure:

1. **Source text** — the raw input string, e.g., `"3 + 4 * 2"`
2. **Lexical analysis** — groups characters into tokens: `NUMBER(3.0)`, `PLUS`, `NUMBER(4.0)`, `TIMES`, `NUMBER(2.0)`
3. **Syntactic analysis** — arranges tokens into a parse tree (concrete syntax tree) that encodes grammatical structure
4. **Semantic analysis / evaluation** — extracts meaning from the tree, producing a typed result (in a calculator: `Double`)

Some compilers add a fifth stage — code generation — that emits machine code or bytecode. Alpaca stops at stage 4: its pipeline produces a typed Scala value, not machine code.

## Alpaca's Pipeline

With Alpaca, running the full pipeline takes two calls:

```scala sc:nocompile
// Full pipeline: source text → typed result
val (_, lexemes) = CalcLexer.tokenize("3 + 4 * 2")
// lexemes: List[Lexeme] — NUMBER(3.0), PLUS, NUMBER(4.0), TIMES, NUMBER(2.0)

val (_, result) = CalcParser.parse(lexemes)
// result: Double | Null = 11.0
```

`CalcLexer.tokenize` handles stages 1–2: it takes the source string and produces a `List[Lexeme]`. `CalcParser.parse` handles stages 3–4: it takes those lexemes, builds the parse tree internally, and returns the typed result.

Both `CalcLexer` and `CalcParser` are objects generated by Alpaca's macros. Their definitions live in separate files (see the cross-links at the bottom of this page).

## Compile-time vs Runtime Boundary

Alpaca draws a sharp line between what happens at compile time and what happens at runtime. This is the most important thing to understand about the library.

> **Compile-time processing:** When you write a `lexer` definition, the Scala 3 macro validates your regex patterns, checks for shadowing, and generates the `Tokenization` object. When you write a `Parser` definition, the macro reads your grammar, builds the LR(1) parse table, and detects any shift/reduce conflicts — all at compile time. At runtime, `tokenize(input)` and `parse(lexemes)` execute the pre-generated code.

In concrete terms:

**Compile time:**
- The `lexer` macro validates regex patterns, detects shadowing (where one pattern makes another unreachable), and emits a `Tokenization` object
- The `Parser` macro reads every `Rule` declaration, constructs the LR(1) parse table, and reports any shift/reduce or reduce/reduce conflicts as compile errors

**Runtime:**
- `tokenize(input)` executes the pre-generated code and returns `List[Lexeme]`
- `parse(lexemes)` executes the pre-built parse table and returns the typed result

The consequence: if your regex is invalid, or your grammar is ambiguous, you get a compile error — not a runtime crash. The pipeline is safe by construction before it ever runs on real input.

Alpaca covers stages 1–3 of the classical pipeline. The "code generation" stage is not part of the library — your Scala semantic actions in the parser rules produce the final typed value directly.

## Formal Definition

> **Definition — Compilation pipeline:**
> A compiler pipeline is a composition of transformations f₁ ∘ f₂ ∘ ... ∘ fₙ where each fᵢ maps the output of fᵢ₋₁ to a more structured representation.
> Alpaca's pipeline: `parse ∘ tokenize : String → R` where R is the root non-terminal's result type.

For the calculator example, `R` is `Double`. For a JSON parser, `R` might be `Any` or a custom AST type. The pipeline shape is always the same; only the result type changes.

The parser internally appends a special `Lexeme.EOF` marker to the lexeme list before running the shift/reduce loop. This is an implementation detail — you do not need to add it yourself.

## Mapping the Stages to Alpaca Types

Each pipeline stage corresponds to a concrete Alpaca type:

| Stage | Input | Output | Alpaca Type |
|-------|-------|--------|-------------|
| Source text | — | `String` | `String` (plain Scala) |
| Lexical analysis | `String` | token stream | `List[Lexeme]` |
| Syntactic analysis | `List[Lexeme]` | parse tree (internal) | LR(1) stack (internal) |
| Semantic analysis | parse tree | typed result | `R \| Null` (your root type) |

The parse tree is never exposed directly — Alpaca builds it internally and immediately evaluates your semantic actions (the `=>` expressions in `rule` definitions). What you get back from `parse` is the final typed value, not an intermediate tree.

## What Comes Next

The rest of the Compiler Theory Tutorial builds on this mental model:

- Next: [Tokens & Lexemes](tokens.md) — what the lexer produces: token classes, token instances, and how they are represented in Alpaca
- [The Lexer: Regex to Finite Automata](lexer-fa.md) — how regular expressions define token classes and how Alpaca compiles them

For the full API, see the reference pages:

- See [Lexer](../lexer.md) for how `CalcLexer` is defined.
- See [Parser](../parser.md) for how `CalcParser` is defined and how grammar rules produce a typed result.
